
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Numerical Optimization applied to Logistic Regression &#8212; Numerical Optimization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Numerical_Optimization_Logistic_Regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Numerical Optimization" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Numerical Optimization - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Numerical Optimization - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Numerical Optimization
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><strong>Numerical Optimization applied to Logistic Regression</strong></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/FabioScielzoOrtiz/Numerical_Optimization_Logistic_Regression" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/FabioScielzoOrtiz/Numerical_Optimization_Logistic_Regression/issues/new?title=Issue%20on%20page%20%2FNumerical_Optimization_Logistic_Regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Numerical_Optimization_Logistic_Regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Numerical Optimization applied to Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements"><strong>Requirements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-logistic-regression"><strong>Binary Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#framework"><strong>Framework</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-define-p-r-x-i"><strong>How to define <span class="math notranslate nohighlight">\(P_r(x_i)\)</span> ?</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-approach-linear-regression"><strong>Naive approach: Linear Regression</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-approach"><strong>Logistic Regression approach</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-estimation"><strong>Model estimation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-predictions"><strong>Model predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-p-r-x-i">Predicting <span class="math notranslate nohighlight">\(P_r(x_i)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-mathcal-y">Predicting <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimization-problem-to-solve">The optimization problem to solve</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization-algorithms"><strong>Numerical optimization algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Framework</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-conditions"><strong>Optimality conditions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-and-global-solutions"><strong>Local and Global solutions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-important-numerical-optimization-algorithms"><strong>Most important Numerical Optimization algorithms</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-logistic-regression"><strong>Optimizing Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initial-elements"><strong>Initial elements</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-and-scipy-optimization"><strong><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-implementation-of-numerical-optimization-algorithms"><strong>Self-implementation of numerical optimization algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-method"><strong>Gradient Descent Method</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-descent-method">Coordinate Descent Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-mini-batch">Stochastic Gradient Descent (Mini-Batch)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-method">Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-newton-method">Stochastic Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-newton-method">Quasi-Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-quasi-newton-method">Stochastic Quasi-Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameters-optimization-hpo"><strong>Hyper-parameters Optimization (HPO)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-gradient-descent"><strong>HPO for Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-gradient-descent-mini-batch"><strong>HPO for Stochastic Gradient Descent (Mini-Batch)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-coordinate-descent"><strong>HPO for Coordinate Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-newton"><strong>HPO for Stochastic Newton</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-quasi-newton"><strong>HPO for Quasi Newton</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-quasi-newton"><strong>HPO for Stochastic Quasi Newton</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-convergence"><strong>Analysis of convergence</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-gradient-descent"><strong>Convergence of Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-coordinate-descent"><strong>Convergence of Coordinate Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-gradient-descent"><strong>Convergence of Stochastic Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-gradient-descent-with-momentum"><strong>Convergence of Stochastic Gradient Descent with Momentum</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-newton-s-method"><strong>Convergence of Newton’s Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-newton-s-method"><strong>Convergence of Stochastic Newton’s Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-quasi-newton-s-bfgs-method"><strong>Convergence of Quasi-Newton’s BFGS Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-quasi-newton-s-sr1-method"><strong>Convergence of Quasi-Newton’s SR1 Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-convergence-of-quasi-newton-s-bfgs-method"><strong>Stochastic Convergence of Quasi-Newton’s BFGS Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-convergence-of-quasi-newton-s-sr1-method"><strong>Stochastic Convergence of Quasi-Newton’s SR1 Method</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-comparison-optimal-values-vs-times"><strong>Methods Comparison: Optimal values vs Times</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-the-sample-size-effect-in-some-of-the-stochastic-methods"><strong>Analysis of the sample size effect in some of the stochastic methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent"><strong>Stochastic Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-newton-s-method"><strong>Stochastic Newton’s Method</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-penalized-logistic-regression"><strong>Optimizing Penalized Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Initial elements</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo"><strong>HPO</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-lambda-influence-in-the-constraint-compliance"><strong>Analysis of lambda influence in the constraint compliance</strong></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="numerical-optimization-applied-to-logistic-regression">
<h1><strong>Numerical Optimization applied to Logistic Regression</strong><a class="headerlink" href="#numerical-optimization-applied-to-logistic-regression" title="Link to this heading">#</a></h1>
<section id="requirements">
<h2><strong>Requirements</strong><a class="headerlink" href="#requirements" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="c1"># from autograd import grad, hessian</span>
<span class="c1"># import autograd.numpy as np</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span> 
</pre></div>
</div>
</div>
</div>
</section>
<section id="binary-logistic-regression">
<h2><strong>Binary Logistic Regression</strong><a class="headerlink" href="#binary-logistic-regression" title="Link to this heading">#</a></h2>
<section id="framework">
<h3><strong>Framework</strong><a class="headerlink" href="#framework" title="Link to this heading">#</a></h3>
<ul>
<li><p>We have a <strong>binary</strong>  response variable <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> with domain <span class="math notranslate nohighlight">\(\lbrace 0, 1\rbrace\)</span>, and a <span class="math notranslate nohighlight">\(p\)</span> predictors <span class="math notranslate nohighlight">\(\mathcal{X}_1,\dots,\mathcal{X}_p\)</span>.</p></li>
<li><p>We have data  for the response and the predictors: <span class="math notranslate nohighlight">\(Y\in \mathbb{R}^n , \hspace{0.1cm} X\in \mathbb{R}^{n\times p}\)</span>.</p></li>
<li><p>We are interestinfg in predict the probabilities of the response for specific individuals, based on their data of the predictors:</p>
<div class="math notranslate nohighlight">
\[P_r(x_i)=P(\mathcal{Y}=r | \mathcal{X}=x_i)\hspace{0.05cm},\quad i=1,\dots,n\]</div>
<p>Where: <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^p\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(r=0,1\)</span></p>
</li>
</ul>
</section>
<section id="how-to-define-p-r-x-i">
<h3><strong>How to define <span class="math notranslate nohighlight">\(P_r(x_i)\)</span> ?</strong><a class="headerlink" href="#how-to-define-p-r-x-i" title="Link to this heading">#</a></h3>
<p>There are two classic approaches, the naive one, based on Linear Regression and the one adopted in Logistic Regression.</p>
<section id="naive-approach-linear-regression">
<h4><strong>Naive approach: Linear Regression</strong><a class="headerlink" href="#naive-approach-linear-regression" title="Link to this heading">#</a></h4>
<p>Under this approach:</p>
<div class="math notranslate nohighlight">
\[P_0(x_i) = \beta_0 + x_i^\prime \cdot \beta\]</div>
<div class="math notranslate nohighlight">
\[P_1(x_i) = 1 - P_0(x_i)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\beta = (\beta_1,\dots , \beta_p)^\prime \in \mathbb{R}^p\)</span>.</p>
<p>We can estimate <span class="math notranslate nohighlight">\(\beta_0, \beta\)</span> by Ordinary Least Squares, as usually in Linear Regression. Then we obtain the estimations <span class="math notranslate nohighlight">\(\widehat{\beta}_0, \widehat{\beta}\)</span>, and using them que can estimate the above probabilities:</p>
<div class="math notranslate nohighlight">
\[\widehat{P}_0(x_i) = \widehat{\beta}_0 + x_i^\prime \cdot \widehat{\beta}\]</div>
<div class="math notranslate nohighlight">
\[\widehat{P}_1(x_i) = 1 - \widehat{P}_0(x_i)\]</div>
<p><strong>Problem of this approach:</strong></p>
<p>There is no guarantee of <span class="math notranslate nohighlight">\(\widehat{P}_r(x_i) \in [0,1]\)</span> and since they are estimations of probabilities, this is not reasonable at all.</p>
<p><strong>Solution:</strong> Logistic Regression.</p>
</section>
<section id="logistic-regression-approach">
<h4><strong>Logistic Regression approach</strong><a class="headerlink" href="#logistic-regression-approach" title="Link to this heading">#</a></h4>
<p>In this approach:</p>
<div class="math notranslate nohighlight">
\[P_1(x_i) =  \dfrac{e^{\beta_0 + x_i^\prime \cdot \beta}}{1+e^{\beta_0 + x_i^\prime \cdot \beta}}\]</div>
<div class="math notranslate nohighlight">
\[P_0(x_i) =1-P_1(x_i) = \dfrac{1}{1+e^{\beta_0 + x_i^\prime \cdot \beta}}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[f(x)=\dfrac{e^x}{1+e^x} \in [0,1], \hspace{0.1cm}\forall x\in \mathbb{R}\]</div>
<p>is the logistic function.</p>
</section>
</section>
<section id="model-estimation">
<h3><strong>Model estimation</strong><a class="headerlink" href="#model-estimation" title="Link to this heading">#</a></h3>
<p>The Maximum Likelihood Method is used.</p>
<p>Consider a s.r.s. <span class="math notranslate nohighlight">\(\mathcal{Y}_1,\dots,\mathcal{Y}_n\)</span> of <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathcal{Y} | \mathcal{X}=x_i \sim Ber(P_1(x_i))\)</span>, then, because of they are identically distributed to <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, is fulfilled:</p>
<div class="math notranslate nohighlight">
\[\mathcal{Y}_i| \mathcal{X}=x_i \sim Ber(P_1(x_i))\hspace{0.05cm}, \quad \forall j=1,\dots,n\]</div>
<p>The density function of <span class="math notranslate nohighlight">\(\mathcal{Y}_i| \mathcal{X}=x_i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[f_{\mathcal{Y}_i| \mathcal{X}=x_i }(y) = P_1(x_i)^{y} \cdot P_0(x_i)^{1-y}\]</div>
<p>Where <span class="math notranslate nohighlight">\(y=0,1\)</span>.</p>
<p>The Likelihood function of the simple random sample (s.r.s.) is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\beta_0,\beta) = f_{\mathcal{Y}_1,\dots,\mathcal{Y}_n| \mathcal{X}=x_i}(y_1,\dots, y_n) = \prod_{i=1}^n f_{\mathcal{Y}_i| \mathcal{X}=x_i }(y_i) = \prod_{j=1}^n  P_1(x_i; \beta_0,\beta)^{y_i} \cdot P_0(x_i; \beta_0,\beta)^{1-y_i}\]</div>
<p>The log Likelihood is given by:</p>
<p>$$log(\mathcal{L}(\beta_0, \beta)) = log\left( \prod_{i=1}^n  P_1(x_i; \beta_0,\beta)^{y_i} \cdot P_0(x_i; \beta_0,\beta)^{1-y_i} \right) = \sum_{i=1}^n log\left(P_1(x_i; \beta_0,\beta)^{y_i} \cdot P_0(x_i; \beta_0,\beta)^{1-y_i}\right) = \[0.5cm]
= \hspace{0.1cm} \sum_{i=1}^n \hspace{0.05cm} y_i\cdot log\big(P_1(x_i; \beta_0,\beta)\big) \hspace{0.1cm} + \hspace{0.1cm} \sum_{i=1}^n \hspace{0.05cm} (1-y_i)\cdot log\big(1- P_1(x_i; \beta_0,\beta)\big)</p>
<p>The estimation of <span class="math notranslate nohighlight">\(\beta_0 , \beta\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{\beta}_0 , \widehat{\beta} \hspace{0.1cm} =\hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta_0, \beta}{Max} \hspace{0.1cm} \mathcal{L}(\beta_0,\beta) \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta_0, \beta}{Max} \hspace{0.1cm} log(\mathcal{L}(\beta_0, \beta)) \\[0.4cm]
\hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta_0, \beta}{Max} \hspace{0.1cm} \Big[\sum_{i=1}^n \hspace{0.05cm} y_i\cdot log\big(P_1(x_i; \beta_0,\beta)\big) \hspace{0.1cm} + \hspace{0.1cm} \sum_{i=1}^n \hspace{0.05cm} (1-y_i)\cdot log\big( 1-P_1(x_i; \beta_0,\beta)\big) \Big] \\[0.4cm]
\hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta_0, \beta}{Max} \hspace{0.1cm} \Big[\sum_{i=1}^n \hspace{0.05cm} y_i\cdot log\left(\dfrac{e^{\beta_0 + x_i^\prime \cdot \beta}}{1+e^{\beta_0 + x_i^\prime \cdot \beta}}\right) \hspace{0.1cm} + \hspace{0.1cm} \sum_{i=1}^n \hspace{0.05cm} (1-y_i)\cdot log\left(\dfrac{1}{1+e^{\beta_0 + x_i^\prime \cdot \beta}}\right)  \Big]  \quad (1)\end{split}\]</div>
<p>Observation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\beta_0 + x_i^\prime \cdot \beta = (1,x_i^\prime)\cdot (\beta_0, \beta^\prime)^\prime = (1, x_{i1},\dots, x_{ip})\cdot \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \dots \\
    \beta_p
\end{pmatrix}\end{split}\]</div>
<p>An this is what we will use in the coding part.</p>
</section>
<section id="model-predictions">
<h3><strong>Model predictions</strong><a class="headerlink" href="#model-predictions" title="Link to this heading">#</a></h3>
<p>We have two options:</p>
<ol class="arabic simple">
<li><p>Predict <span class="math notranslate nohighlight">\(P_r(x_i)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <strong>Regression</strong></p></li>
<li><p>Predict <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>  <span class="math notranslate nohighlight">\(\hspace{0.1cm} \Rightarrow \hspace{0.1cm}\)</span> <strong>Classification</strong></p></li>
</ol>
<section id="predicting-p-r-x-i">
<h4>Predicting <span class="math notranslate nohighlight">\(P_r(x_i)\)</span><a class="headerlink" href="#predicting-p-r-x-i" title="Link to this heading">#</a></h4>
<p>Given <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span>,</p>
<div class="math notranslate nohighlight">
\[\widehat{P}_0(x_i) = \dfrac{e^{\widehat{\beta}_0 + x_i^\prime \cdot \widehat{\beta}}}{1+e^{\widehat{\beta}_0 + x_i^\prime \cdot \widehat{\beta}}}\]</div>
<div class="math notranslate nohighlight">
\[\widehat{P}_1(x_i) = 1 - \widehat{P}_0(x_i) = \dfrac{1}{1+e^{\widehat{\beta}_0 + x_i^\prime \cdot \widehat{\beta}}}\]</div>
</section>
<section id="predicting-mathcal-y">
<h4>Predicting <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span><a class="headerlink" href="#predicting-mathcal-y" title="Link to this heading">#</a></h4>
<p>Given <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span>,</p>
<div class="math notranslate nohighlight">
\[\widehat{y}_i \hspace{0.1cm}=\hspace{0.1cm} arg \hspace{0.1cm}\underset{r=0,1}{Max} \hspace{0.1cm}\widehat{P}_r(x_i)\]</div>
<p>In other words, <span class="math notranslate nohighlight">\(\widehat{y}_i\)</span> is the category with higher predicted probability for <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
</section>
</section>
<section id="the-optimization-problem-to-solve">
<h3>The optimization problem to solve<a class="headerlink" href="#the-optimization-problem-to-solve" title="Link to this heading">#</a></h3>
<p>The optimization problem that we are going to solve along this project is (1), but expressing it as a minimization problem an in a matrix form, since its the most efficient computational way to address it.</p>
<p>Now we are going to change the definition of some terms a little bit:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta \leftarrow (\beta_o, \beta^\prime)=(\beta_0, \beta_1, \dots, \beta_p)^\prime\)</span></p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X \leftarrow (1, X)\)</span></p>
<ul>
<li><p>Therefore, now: <span class="math notranslate nohighlight">\(x_i \leftarrow (1, x_i^\prime)^\prime\)</span></p></li>
</ul>
</li>
</ul>
<p>Taking this into account, the optimization problem that we want to solve is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{\beta} \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Max} \hspace{0.1cm} log(\mathcal{L}(\beta)) \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta_0, \beta}{Min} \hspace{0.1cm} - log(\mathcal{L}(\beta)) =  \\[0.4cm]
\hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Min} \hspace{0.1cm} - \Big[\sum_{i=1}^n \hspace{0.05cm} y_i\cdot log\left(\dfrac{e^{ x_i^\prime \cdot \beta}}{1+e^{ x_i^\prime \cdot \beta}}\right) \hspace{0.1cm} + \hspace{0.1cm} \sum_{i=1}^n \hspace{0.05cm} (1-y_i)\cdot log\left(\dfrac{1}{1+e^{ x_i^\prime \cdot \beta}}\right)  \Big]  =  \\[0.4cm]
\hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Min} \hspace{0.1cm} -\Big[\hspace{0.05cm}   Y \cdot log(P_1) + (1-Y)\cdot log(1-P_1) \hspace{0.05cm} \Big]
\end{split}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[P_1 \hspace{0.06cm} = \hspace{0.06cm}  \dfrac{e^{ X \cdot \beta}}{1+e^{ X \cdot \beta}}\hspace{0.06cm} = \hspace{0.06cm}  \big(P_1(x_i) : i=1,\dots, n\big)^\prime\]</div>
<p>The <strong>gradient</strong> of <span class="math notranslate nohighlight">\(f(\beta) = - log(\mathcal{L}(\beta))\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\nabla f(\beta) \hspace{0.06cm} = \hspace{0.06cm}   X^\prime \cdot (P_1-Y)\]</div>
<p>The <strong>hessian</strong> of <span class="math notranslate nohighlight">\(f(\beta) = - log(\mathcal{L}(\beta))\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\nabla^2 f(\beta) \hspace{0.06cm} = \hspace{0.06cm}   X^\prime \cdot D \cdot X\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[D= diag(P_1\cdot (1-P_1))\]</div>
<p>Some documentation to support this part: <a class="reference external" href="https://stats.stackexchange.com/questions/68391/hessian-of-logistic-function">https://stats.stackexchange.com/questions/68391/hessian-of-logistic-function</a></p>
</section>
</section>
<section id="numerical-optimization-algorithms">
<h2><strong>Numerical optimization algorithms</strong><a class="headerlink" href="#numerical-optimization-algorithms" title="Link to this heading">#</a></h2>
<section id="id1">
<h3><strong>Framework</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>We are focused in the non-linear optimization scenario, whose framework is the following:</p>
<div class="math notranslate nohighlight">
\[\underset{x\in A}{Min} f(x) \hspace{0.3cm}\big(\equiv\hspace{0.1cm}  \underset{x\in A}{Max} - f(x) \hspace{0.05cm} \big) \hspace{0.3cm} (\text{I})\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f: D_f \subset \mathbb{R}^p \rightarrow \mathbb{R}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A\subset \mathbb{R}^p\)</span> contains the constraints imposed to <span class="math notranslate nohighlight">\(x\)</span>. Which could be <span class="math notranslate nohighlight">\(A=\mathbb{R}^p\)</span> if it is an unconstrained problem.</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> has continuous second order derivatives.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x) &gt; \infty, \hspace{0.15cm} \forall x \in A\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\underset{x \rightarrow \infty}{lim} f(x) = \infty\)</span></p></li>
</ul>
</section>
<section id="optimality-conditions">
<h3><strong>Optimality conditions</strong><a class="headerlink" href="#optimality-conditions" title="Link to this heading">#</a></h3>
<p>The optimality conditions in this scenario are the following:</p>
<ul class="simple">
<li><p><strong>First-order necessary conditions:</strong></p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(x^*\)</span> is a local minimizer  <span class="math notranslate nohighlight">\(\Rightarrow\)</span> <span class="math notranslate nohighlight">\(\nabla f(x^*)=0\quad (NC1)\)</span></p></li>
</ul>
</li>
<li><p><strong>Second-order necessary conditions:</strong></p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(x^*\)</span> is a local minimizer <span class="math notranslate nohighlight">\(\Rightarrow\)</span> <span class="math notranslate nohighlight">\(\nabla^2 f(x^*) \succeq 0\quad (NC2)\)</span></p></li>
</ul>
</li>
</ul>
<p>The importance of the necessary condition is summarized as follows:</p>
<ul class="simple">
<li><p>The local minimizers of <span class="math notranslate nohighlight">\(f\)</span> are contain in <span class="math notranslate nohighlight">\(NC1 \cap NC2\)</span>, namely, in the set of points <span class="math notranslate nohighlight">\(x\)</span> that satisfy <span class="math notranslate nohighlight">\(NC1\)</span> and <span class="math notranslate nohighlight">\(NC2\)</span> simultaneously.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Second-order sufficient conditions:</strong></p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(\hspace{0.1cm}\begin{cases} \nabla f(x^*)=0 \hspace{0.2cm} (SC1) \\[0.1cm] \nabla^2 f(x^*)\succ 0 \hspace{0.2cm} (SC2)\end{cases}\)</span>
<span class="math notranslate nohighlight">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <span class="math notranslate nohighlight">\(x^*\)</span> is a  local minimizer.</p></li>
</ul>
</li>
</ul>
<p><strong>Definite property for matrices:</strong></p>
<p>Given an square matrix <span class="math notranslate nohighlight">\(M_{p\times p}\)</span> (like the hessian), with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \dots , \lambda_p\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(M\)</span> is positive definite <span class="math notranslate nohighlight">\((M\succ 0)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.12cm}\Leftrightarrow\hspace{0.12cm}\)</span> <span class="math notranslate nohighlight">\(\lambda_i &gt; 0, \forall i=1,\dots ,p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> is semi positive definite <span class="math notranslate nohighlight">\((M\succeq 0)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.12cm}\Leftrightarrow\hspace{0.12cm}\)</span> <span class="math notranslate nohighlight">\(\lambda_i \geq 0, \forall i=1,\dots ,p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> is negative definite <span class="math notranslate nohighlight">\((M\prec 0)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.12cm}\Leftrightarrow\hspace{0.12cm}\)</span> <span class="math notranslate nohighlight">\(\lambda_i &lt; 0, \forall i=1,\dots ,p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> is semi negative definite <span class="math notranslate nohighlight">\((M\preceq 0)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.12cm}\Leftrightarrow\hspace{0.12cm}\)</span> <span class="math notranslate nohighlight">\(\lambda_i \leq 0, \forall i=1,\dots ,p\)</span></p></li>
</ul>
<p><strong>Singular points:</strong></p>
<p>They are local minimizers <span class="math notranslate nohighlight">\(x^*\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[\nabla f(x^*)=\nabla^2 f(x^*)=0\]</div>
<p>In other words, they are local minimizer that don’t satisfy the sufficient conditions.</p>
<p>These point are very difficult to explore.</p>
</section>
<section id="local-and-global-solutions">
<h3><strong>Local and Global solutions</strong><a class="headerlink" href="#local-and-global-solutions" title="Link to this heading">#</a></h3>
<p><strong>Why we talk about local solutions but not global?</strong></p>
<ul class="simple">
<li><p>Globals solutions are in general difficult to identify and to locate, except in convex optimization (but our optimization problem is not convex).</p></li>
<li><p>Local solution are “easy” to find (much more than global ones) , but there is not guarantee to be the best.</p></li>
</ul>
<p><strong>Recap about local and global solutions:</strong></p>
<ul>
<li><p><strong>Local solutions:</strong></p>
<p><span class="math notranslate nohighlight">\(x^*\)</span> is a local solution of <span class="math notranslate nohighlight">\((\text{I})\)</span> if and only if:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x^* \in A,\hspace{0.1cm}\)</span> i.e. <span class="math notranslate nohighlight">\(x^*\)</span> is feasible.</p></li>
<li><p><span class="math notranslate nohighlight">\(\exists \hspace{0.1cm} \varepsilon &gt;0, \hspace{0.1cm} \forall x\in B(x^*, \epsilon)\cup A,\hspace{0.1cm} f(x^*) \leq f(x)\)</span></p></li>
</ul>
<p>where: <span class="math notranslate nohighlight">\(\hspace{0.1cm}B(x*, \varepsilon) = (x^*-\epsilon, \hspace{0.1cm} x^*+\varepsilon)\)</span></p>
</li>
</ul>
<ul>
<li><p><strong>Global solutions:</strong></p>
<p><span class="math notranslate nohighlight">\(x^*\)</span> is a global solution of <span class="math notranslate nohighlight">\((\text{I})\)</span> if and only if:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x^* \in A,\hspace{0.1cm}\)</span> i.e. <span class="math notranslate nohighlight">\(x^*\)</span> is feasible.</p></li>
<li><p><span class="math notranslate nohighlight">\(\forall x\in A,\hspace{0.1cm} f(x^*) \leq f(x)\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="most-important-numerical-optimization-algorithms">
<h3><strong>Most important Numerical Optimization algorithms</strong><a class="headerlink" href="#most-important-numerical-optimization-algorithms" title="Link to this heading">#</a></h3>
<p>The main goal of numerical optimization algorithms is to find a vector of decision variables <span class="math notranslate nohighlight">\(x^*\)</span> close enough to be a local solution of optimization problems. In the non-linear case this mains to find <span class="math notranslate nohighlight">\(x^*\)</span> close enough to satisfy the (local) sufficient conditions seen above.</p>
<p>In the following section section we are going to implement several numerical optimization algorithm, that are specially powerful to solve non-linear optimization problems, like our problem.</p>
<p>The implemented algorithms (methods) are the following:</p>
<ul class="simple">
<li><p>Gradient Descent</p></li>
<li><p>Coordinate Descent</p></li>
<li><p>Stochastic Gradient Descent (Mini-Batch)</p></li>
<li><p>Newton Method</p></li>
<li><p>Stochastic Newton Method (Hessian-Free Inexact Newton)</p></li>
<li><p>Quasi-Newton Method</p></li>
<li><p>Stochastic Quasi-Newton Method</p></li>
</ul>
<p>The basic iterative procedure of these algorithms is:</p>
<div class="math notranslate nohighlight">
\[x_{k+1} = x_{k} + \alpha_k \cdot p_k, \quad \small{k=0,1,2,\dots}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the index that represent the algorithm iterations.</p></li>
<li><p><span class="math notranslate nohighlight">\(p_k\)</span> is the search direction.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_k\)</span> the step-length.</p></li>
</ul>
<p>These algorithms try to move <span class="math notranslate nohighlight">\(f\)</span> closer to a local solution with each iteration, until its value converge to a certain value.</p>
<p>It’s important that the algorithm has a convergence to a local solution regardless of the initial point <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p><strong>Definition of <span class="math notranslate nohighlight">\(p_k\)</span></strong></p>
<p>Depend on the specific algorithm.</p>
<ul>
<li><p><strong>Gradien descent</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - \nabla f(x_k)\]</div>
</li>
</ul>
<ul>
<li><p><strong>Newtons method</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - (\nabla^2 f(x_k))^{-1}\cdot \nabla f(x_k)\]</div>
</li>
</ul>
<ul>
<li><p><strong>Quasi-Newton’s method</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - B_k^{-1} \cdot \nabla f(x_k)\]</div>
<ul>
<li><p>BFGS rule:</p>
<div class="math notranslate nohighlight">
\[B_{k+1} = B_k - \dfrac{(B_k \cdot s_k)\cdot (B_k\cdot s_k)^\prime}{s_k^\prime \cdot B_k \cdot s_k} + \dfrac{y_k \cdot y_k^\prime}{y_k^\prime \cdot s_k}\]</div>
</li>
<li><p>Symmetric rank-one rule:</p>
<div class="math notranslate nohighlight">
\[B_{k+1} = B_k - \dfrac{(y_k - B_k \cdot s_k)\cdot (y_k - B_k \cdot s_k)^\prime}{(y_k - B_k \cdot s_k)^\prime \cdot s_k} \]</div>
</li>
</ul>
<p>where: <span class="math notranslate nohighlight">\(s_k = x_{k+1} - x_k\)</span> and <span class="math notranslate nohighlight">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_k)\)</span>.</p>
</li>
</ul>
<ul>
<li><p><strong>Coordinate Descent</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - \nabla_{j(k)} f(x_k) \cdot e_{j(k)}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e_{j(k)}\)</span> is the <span class="math notranslate nohighlight">\(j(k)\)</span>-th canonical vector of size <span class="math notranslate nohighlight">\(p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(j(k)\)</span> is chosen randomly among <span class="math notranslate nohighlight">\([0,p]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{j(k)} f(x_k) \hspace{0.05cm}=\hspace{0.05cm} \dfrac{\partial f(x_k)}{\partial x_{k_{j(k)}}}\)</span></p></li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Stochastic Gradient Descent (Mini-Batch)</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - \widetilde{\nabla} f(x_k)_{S_k}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\widetilde{\nabla} f(x_k)_{S_k}\)</span> is an estimation of <span class="math notranslate nohighlight">\(\nabla f(x_k)\)</span> using a random sample  <span class="math notranslate nohighlight">\(S_k\)</span> of the data <span class="math notranslate nohighlight">\((X,Y)\)</span>, one in each iteration.</p></li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Stochastic Gradient Descent (Mini-Batch with momentum)</strong></p>
<div class="math notranslate nohighlight">
\[x_{k+1} = x_{k} + \alpha_k \cdot p_k + m_k, \quad \small{k=0,1,2,\dots}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}m_k = \begin{cases} \gamma \cdot (x_k - x_{k-1}), \hspace{0.2cm} \small{k=1,2,\dots} \\
      0, \hspace{0.2cm} \small{k=0} \end{cases}\end{split}\]</div>
</li>
</ul>
<ul>
<li><p><strong>Stochastic Newton Method</strong></p>
<div class="math notranslate nohighlight">
\[p_k = - \left(\widetilde{\nabla}^2 f(x_k)_{S_k^H}\right)^{-1} \cdot \widetilde{\nabla} f(x_k)_{S_k}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_k\)</span> is a sample of the data <span class="math notranslate nohighlight">\((X,Y)\)</span> used to estimate the gradient.</p></li>
<li><p><span class="math notranslate nohighlight">\(S_k^H\)</span> is another sample of the data <span class="math notranslate nohighlight">\((X,Y)\)</span> used to estimate the hessian.</p></li>
</ul>
</li>
</ul>
<p><strong>Definition of <span class="math notranslate nohighlight">\(\alpha_k\)</span>: Armijo Rule</strong></p>
<p>Initialize <span class="math notranslate nohighlight">\(\alpha_0\)</span></p>
<p>For <span class="math notranslate nohighlight">\(k=0,1,2,\dots\)</span></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\hspace{0.1cm}f(x_{k} + \alpha_k \cdot p_k) \hspace{0.1cm}&gt;\hspace{0.1cm} f(x_k) + \sigma \cdot \alpha_k\cdot (p_k^\prime \cdot p_k)\)</span> <span class="math notranslate nohighlight">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <span class="math notranslate nohighlight">\(\alpha_{k+1} = \alpha_k \cdot \phi^{k+1}\)</span></p></li>
<li><p>Else <span class="math notranslate nohighlight">\(\hspace{0.15cm}\Rightarrow\hspace{0.15cm}\)</span> <span class="math notranslate nohighlight">\(\alpha_{k+1}=\alpha_k\)</span></p></li>
</ul>
<p>Usually values for the parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi \in \left[0.1 , \hspace{0.08cm} 0.5\right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma \in \left[10^{-5}, \hspace{0.08cm} 10^{-1}\right]\)</span></p></li>
</ul>
</section>
</section>
<section id="optimizing-logistic-regression">
<h2><strong>Optimizing Logistic Regression</strong><a class="headerlink" href="#optimizing-logistic-regression" title="Link to this heading">#</a></h2>
<section id="initial-elements">
<h3><strong>Initial elements</strong><a class="headerlink" href="#initial-elements" title="Link to this heading">#</a></h3>
<p>In this section we are going to define some important elements that are strictly necessary to solve our optimization problem with some of the numerical optimization algorithms that we will use. These are:</p>
<ul class="simple">
<li><p>The objective function</p></li>
<li><p>The gradient of the objective function</p></li>
<li><p>The hessian of the objective function</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Logistic function</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">def logistic(x):</span>
<span class="sd">    return np.exp(x) / (1+np.exp(x))</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># To avoid numerical problems we will use the Scipy implementations of the logistic function</span>
<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># Get data</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="p">))</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span> <span class="c1"># (1,X)</span>
    <span class="n">initial_betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (beta_0, beta)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">initial_betas</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">arr</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">func1d</span><span class="o">=</span><span class="n">logistic</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">initial_betas</span>

<span class="c1"># Objective function in element format</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">def minus_logL(betas, X, Y):</span>
<span class="sd">    n = len(X)</span>
<span class="sd">    p1 = np.e**(X @ betas) / (1 + np.e**(X @ betas))</span>
<span class="sd">    sum1 = np.sum([Y[i]*np.log(p1[i]) for i in range(0,n)])</span>
<span class="sd">    sum2 = np.sum([(1-Y[i])*np.log(1-p1[i]) for i in range(0,n)])</span>
<span class="sd">    result = - (sum1 + sum2)</span>
<span class="sd">    return result</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># Objective function in matrix format</span>
<span class="k">def</span> <span class="nf">minus_logL</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-15</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">sum1</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">sum2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>  <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">sum1</span> <span class="o">+</span> <span class="n">sum2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="c1"># Gradient of the objective function</span>
<span class="k">def</span> <span class="nf">gradient_function</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">p1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradient</span>

<span class="c1"># Derivative if the Log-Likelihood</span>
<span class="k">def</span> <span class="nf">deriv_function</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">derivative</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">p1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">derivative</span>

<span class="c1"># Gradient with autograd (doesn&#39;t work well with big data, almost in this case)</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">gradient_autograd = grad(minus_logL_autograd, argnum=0) # argnum=0 to indicate that we want the gradient</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># Hessian of the objective function</span>
<span class="k">def</span> <span class="nf">hessian_function</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">p1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">))</span>
    <span class="n">hessian</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">D</span> <span class="o">@</span> <span class="n">X</span>
    <span class="k">return</span> <span class="n">hessian</span>

<span class="c1"># Hessian with autograd (doesn&#39;t work well with big data, almost in this case)</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">hessian_autograd = hessian(minus_logL_autograd, argnum=0) # argnum=0 to indicate that we want the Hessian of minus_logL with respect to the first variable (betas)</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\nhessian_autograd = hessian(minus_logL_autograd, argnum=0) # argnum=0 to indicate that we want the Hessian of minus_logL with respect to the first variable (betas)\n&#39;
</pre></div>
</div>
</div>
</div>
<p>Now we get the data with which we are going to work. Our initial idea was considering real big data to force the algorithms and make the project more realistic, but the computational time needed to run the whole notebook was too much, so that we decided to try with “big data”, but not too much big data, concretely with <span class="math notranslate nohighlight">\(n=30000\)</span> and <span class="math notranslate nohighlight">\(p=25\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">initial_betas</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sklearn-and-scipy-optimization">
<h3><strong><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization</strong><a class="headerlink" href="#sklearn-and-scipy-optimization" title="Link to this heading">#</a></h3>
<p>In this section we are going to solve our optimization problem using the libraries <code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code>, and collect interesting information regarding to it, that will be used in subsequent section more focused in analytics.</p>
<p>These result will be consider as a benchmark (a reference to compare our developments).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">benchmark_methods</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_BFGS&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_L-BFGS-B&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_SLSQP&#39;</span><span class="p">]</span>

<span class="c1"># BFGS := Broyden–Fletcher–Goldfarb–Shanno algorithm</span>
<span class="c1"># Newton-CG := Newton Conjugate Gradient algorithm</span>
<span class="c1"># L-BFGS-B := Limited memory BFGS</span>
<span class="c1"># SLSQP := Sequential Least Squares Programming </span>

<span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">benchmark_methods</span><span class="p">:</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;sklearn&#39;</span><span class="p">:</span>
        <span class="n">logistic_regression</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">logistic_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logistic_regression</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">minus_logL</span><span class="p">(</span><span class="n">logistic_regression</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">:</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">jac</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;x&#39;: array([ 5.28803335,  5.31456709,  5.25690809,  0.02897051, -5.23539119,
        -2.71234298, -5.9467545 ,  2.63827799, -6.58206716,  5.98875997,
         4.58747174,  3.24867487,  0.67856699, -3.25560885,  1.32075729,
         1.29431084, -0.59910036, -6.59266532,  3.30147313, -4.60156969,
        -6.61433056,  4.6290872 ,  3.93758349, -0.56097163,  1.33033932,
         1.93490455]),
 &#39;fun&#39;: 1863.1467847925437,
 &#39;time&#39;: 0.043999671936035156}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_BFGS&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Desired error not necessarily achieved due to precision loss.
  success: False
   status: 2
      fun: 1863.146784791233
        x: [ 5.288e+00  5.315e+00 ...  1.330e+00  1.935e+00]
      nit: 42
      jac: [-1.526e-05  1.526e-05 ...  1.526e-05  0.000e+00]
 hess_inv: [[ 7.377e-03  3.436e-03 ... -1.059e-03  2.509e-03]
            [ 3.436e-03  4.182e-03 ... -5.560e-04  1.533e-03]
            ...
            [-1.059e-03 -5.560e-04 ...  1.933e-03 -4.331e-04]
            [ 2.509e-03  1.533e-03 ... -4.331e-04  2.938e-03]]
     nfev: 2133
     njev: 79
     time: 4.999240159988403
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> message: Optimization terminated successfully.
 success: True
  status: 0
     fun: 1863.146785187762
       x: [ 5.288e+00  5.314e+00 ...  1.330e+00  1.935e+00]
     nit: 8
     jac: [ 1.381e-03  3.739e-03 ...  8.935e-03  2.601e-03]
    nfev: 10
    njev: 10
    nhev: 8
    time: 19.65959930419922
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_L-BFGS-B&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH
  success: True
   status: 0
      fun: 1863.1467847986262
        x: [ 5.288e+00  5.315e+00 ...  1.330e+00  1.935e+00]
      nit: 13
      jac: [ 3.411e-04 -3.183e-04 ... -5.684e-04  1.137e-04]
     nfev: 459
     njev: 17
 hess_inv: &lt;26x26 LbfgsInvHessProduct with dtype=float64&gt;
     time: 0.9834015369415283
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_SLSQP&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> message: Optimization terminated successfully
 success: True
  status: 0
     fun: 1863.1467963523812
       x: [ 5.288e+00  5.314e+00 ...  1.330e+00  1.935e+00]
     nit: 24
     jac: [-1.575e-02 -4.807e-03 ...  1.482e-02  2.016e-02]
    nfev: 704
    njev: 24
    time: 1.6142446994781494
</pre></div>
</div>
</div>
</div>
</section>
<section id="self-implementation-of-numerical-optimization-algorithms">
<h3><strong>Self-implementation of numerical optimization algorithms</strong><a class="headerlink" href="#self-implementation-of-numerical-optimization-algorithms" title="Link to this heading">#</a></h3>
<p>In this section te code with the implementation fo the above algorithm is available.</p>
<section id="gradient-descent-method">
<h4><strong>Gradient Descent Method</strong><a class="headerlink" href="#gradient-descent-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">gradient_descent_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="p">{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
       
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>

           <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 

                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span> <span class="o">=</span> <span class="n">gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                           <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">gradient_descent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.28918213,  5.31572191,  5.25805256,  0.02897158, -5.23652807,
       -2.71293351, -5.94804864,  2.63885016, -6.58350569,  5.99006144,
        4.58846468,  3.24938207,  0.67871167, -3.25631815,  1.32104386,
        1.29459074, -0.59923171, -6.59410188,  3.3021864 , -4.60257038,
       -6.61576843,  4.63009278,  3.93844096, -0.56109073,  1.33063148,
        1.93532963])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1863.1468283694426
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1366
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">run_time</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15.348620176315308
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([  2040.78392311,  23408.40547349, 322380.92969808, 225064.53577115,
       141911.24907745])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">)</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1863.14683243, 1863.14683138, 1863.14683035, 1863.14682935,
       1863.14682837])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="coordinate-descent-method">
<h3>Coordinate Descent Method<a class="headerlink" href="#coordinate-descent-method" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">coordinate_descent_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">derivative</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">derivative</span> <span class="o">=</span> <span class="n">derivative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="p">{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
       
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
           
           <span class="n">jk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_0</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
           <span class="n">e_jk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_0</span><span class="p">))</span>
           <span class="n">e_jk</span><span class="p">[</span><span class="n">jk</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
           <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">derivative</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">jk</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">*</span> <span class="n">e_jk</span>

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 

                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span> 
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_method</span> <span class="o">=</span> <span class="n">coordinate_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">derivative</span><span class="o">=</span><span class="n">deriv_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">coordinate_method</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_method</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 7.98551747,  8.01798023,  7.94332815,  0.03454666, -7.90859671,
       -4.09092111, -8.9777438 ,  4.        , -9.97612386,  9.        ,
        7.        ,  5.        ,  1.        , -5.        ,  1.99747214,
        1.94083449, -0.91251638, -9.96406837,  4.98816707, -6.94840999,
       -9.988173  ,  7.        ,  5.94388119, -0.82655925,  2.00444824,
        2.92613241])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_method</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2027.7639322358696
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_method</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>35
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-mini-batch">
<h3>Stochastic Gradient Descent (Mini-Batch)<a class="headerlink" href="#stochastic-gradient-descent-mini-batch" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">stochastic_gradient_descent_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_size</span> <span class="o">=</span> <span class="n">sample_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="p">{},{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
            <span class="n">extra_args</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
          
           <span class="c1"># Samples change in each iteration</span>
           <span class="n">seed_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">k</span>
           <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_k</span><span class="p">)</span>
           <span class="n">random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
           <span class="n">X_sample</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">random_indices</span><span class="p">,:]</span>
           <span class="n">Y_sample</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">random_indices</span><span class="p">]</span>

           <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>  <span class="c1"># stochastic gradient approximation </span>

           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
               <span class="n">m</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
           <span class="k">else</span><span class="p">:</span>
               <span class="n">m</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 

                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<ul class="simple">
<li><p>Without momentum</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
                                <span class="n">sample_size</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([  8.04344796,   8.04646635,   8.0504952 ,   0.04418402,
        -7.84305642,  -4.10068646,  -8.94556712,   3.93310477,
        -9.88324396,   8.98416031,   6.96194451,   4.94722316,
         0.9073087 ,  -4.94521276,   2.0269666 ,   1.91346678,
        -0.99789768,  -9.95286513,   5.09444643,  -7.00842046,
       -10.02063994,   6.95580257,   5.99558745,  -0.7717752 ,
         2.0287944 ,   2.87549497])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2039.7202374928115
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>156
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>With momentum</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> 
                                <span class="n">sample_size</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 8.06842281e+00,  8.05270628e+00,  8.00553500e+00,  3.55944835e-03,
       -7.89007942e+00, -4.17231590e+00, -8.96116957e+00,  3.99016301e+00,
       -9.86911746e+00,  9.02952980e+00,  6.97359649e+00,  4.93701271e+00,
        9.48345538e-01, -4.92182109e+00,  1.99465180e+00,  2.00076052e+00,
       -9.53748816e-01, -9.95203900e+00,  5.05002163e+00, -6.95780364e+00,
       -9.99166521e+00,  7.00489543e+00,  5.92786566e+00, -8.27648156e-01,
        1.97207613e+00,  2.83926981e+00])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2033.7296906503555
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45
</pre></div>
</div>
</div>
</div>
</section>
<section id="newton-method">
<h3>Newton Method<a class="headerlink" href="#newton-method" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Newton_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span> <span class="o">=</span> <span class="n">hessian</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="p">{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
       
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>

           <span class="k">try</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  
           <span class="k">except</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 

                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Newton</span> <span class="o">=</span> <span class="n">Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span>  
                        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.28836078,  5.31489012,  5.2572412 ,  0.02896647, -5.23572858,
       -2.71249591, -5.94712449,  2.63844244, -6.58248141,  5.98912155,
        4.58776585,  3.2488893 ,  0.67860376, -3.25582166,  1.32083916,
        1.29439769, -0.59915189, -6.59307617,  3.30167777, -4.6018611 ,
       -6.6147402 ,  4.62937417,  3.93783388, -0.56103004,  1.33042096,
        1.93503561])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1863.1467889446008
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Newton</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Newton</span><span class="o">.</span><span class="n">run_time</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>193.69731044769287
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-newton-method">
<h3>Stochastic Newton Method<a class="headerlink" href="#stochastic-newton-method" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Stochastic_Newton_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span> <span class="o">=</span> <span class="n">hessian</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sample_size</span> <span class="o">=</span> <span class="n">grad_sample_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hess_sample_size</span> <span class="o">=</span> <span class="n">hess_sample_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="p">{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
        
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
            <span class="n">extra_args</span> <span class="o">=</span> <span class="p">[]</span>
       
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
          
           <span class="c1"># Samples change in each iteration</span>
           <span class="n">seed_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">k</span>
           <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_k</span><span class="p">)</span>
           <span class="n">grad_random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
           <span class="n">hess_random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hess_sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
           <span class="n">X_sample_grad</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">grad_random_indices</span><span class="p">,:]</span>
           <span class="n">Y_sample_grad</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">grad_random_indices</span><span class="p">]</span>
           <span class="n">X_sample_hess</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">hess_random_indices</span><span class="p">,:]</span>
           <span class="n">Y_sample_hess</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">hess_random_indices</span><span class="p">]</span>

           <span class="k">try</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_hess</span><span class="p">,</span> <span class="n">Y_sample_hess</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">))</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span> 
           <span class="k">except</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_hess</span><span class="p">,</span> <span class="n">Y_sample_hess</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">))</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>  

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 

                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Newton</span> <span class="o">=</span> <span class="n">Stochastic_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">Stochastic_Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.72538984,  5.66991528,  5.68390622, -0.02807675, -5.61777829,
       -2.88446316, -6.34436095,  2.82183427, -7.01517676,  6.39899183,
        4.96364104,  3.52282222,  0.63622257, -3.50775569,  1.42446114,
        1.36052617, -0.68682871, -7.02087466,  3.61210992, -4.99353931,
       -7.14017658,  4.95085395,  4.24719655, -0.56356911,  1.40554013,
        2.07283688])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1879.4833756892087
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Newton</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>89
</pre></div>
</div>
</div>
</div>
</section>
<section id="quasi-newton-method">
<h3>Quasi-Newton Method<a class="headerlink" href="#quasi-newton-method" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Quasi_Newton_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>  <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span> <span class="o">=</span> <span class="n">hessian</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">=</span> <span class="n">rule</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">{},{},{},{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
        <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
       
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
           
           <span class="k">try</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  
           <span class="k">except</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
           <span class="n">M1</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M2</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">M1</span> <span class="p">;</span> <span class="n">M3</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M4</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M5</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">M1</span>
           
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">==</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">:</span>               
               <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">M1</span> <span class="o">@</span> <span class="n">M1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">M2</span> <span class="o">+</span> <span class="n">M3</span><span class="o">/</span><span class="n">M4</span>
           <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">==</span> <span class="s1">&#39;SR1&#39;</span><span class="p">:</span>
               <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">M5</span><span class="nd">@M5</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">M5</span><span class="o">.</span><span class="n">T</span><span class="nd">@s</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
           
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<ul class="simple">
<li><p>Using BFGS rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                           <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.28508513,  5.31159884,  5.25390471,  0.02870634, -5.23285702,
       -2.71114015, -5.94384551,  2.63667666, -6.57883255,  5.98538907,
        4.58488352,  3.24678227,  0.67798943, -3.25415551,  1.319855  ,
        1.29342289, -0.5989807 , -6.58940455,  3.29953562, -4.59936121,
       -6.61108595,  4.62645972,  3.93530871, -0.5608853 ,  1.32942679,
        1.93367787])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1863.14737262964
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">run_time</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.9510254859924316
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Using SR1 rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                         <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 5.25752651,  5.28385152,  5.22576345,  0.0261718 , -5.20918385,
       -2.70001759, -5.91669395,  2.62167257, -6.54866392,  5.95388488,
        4.56060363,  3.2290382 ,  0.67258427, -3.24059557,  1.31137953,
        1.28508719, -0.59785835, -6.55901099,  3.28140601, -4.57876238,
       -6.5808251 ,  4.60184985,  3.9140122 , -0.56003622,  1.32082836,
        1.92215314])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Quasi_Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1863.2113894233012
</pre></div>
</div>
</div>
</div>
</section>
<section id="stochastic-quasi-newton-method">
<h3>Stochastic Quasi-Newton Method<a class="headerlink" href="#stochastic-quasi-newton-method" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Stochastic_Quasi_Newton_method</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">objective</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span> <span class="o">=</span> <span class="n">hessian</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">=</span> <span class="n">rule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_sample_size</span> <span class="o">=</span> <span class="n">grad_sample_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hess_sample_size</span> <span class="o">=</span> <span class="n">hess_sample_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="c1"># Method to set the parameters of the estimator</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="p">:</span>

        <span class="c1"># args is a tuple with the extra arguments of &#39;objective&#39; and &#39;gradient&#39; functions.</span>
        <span class="c1"># sigma usually in [0.00001, 0.1]</span>
        <span class="c1"># phi usually in [0.1, 0.5]</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">{},{},{},{},{},{}</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_0</span>
        <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="c1"># initial alpha</span>
      
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">args</span>
            <span class="n">extra_args</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">hess_random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hess_sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">X_sample_hess</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">hess_random_indices</span><span class="p">,:]</span>
        <span class="n">Y_sample_hess</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">hess_random_indices</span><span class="p">]</span>

        <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_sample_hess</span><span class="p">,</span> <span class="n">Y_sample_hess</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>

           <span class="c1"># Samples change in each iteration</span>
           <span class="n">seed_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">k</span>
           <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_k</span><span class="p">)</span>
           <span class="n">grad_random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_sample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
           <span class="n">X_sample_grad</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">grad_random_indices</span><span class="p">]</span>
           <span class="n">Y_sample_grad</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">grad_random_indices</span><span class="p">]</span>

           <span class="k">try</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>  
           <span class="k">except</span><span class="p">:</span>
               <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>  

           <span class="c1"># If Armijo rule is NOT fulfilled</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nd">@p</span><span class="p">[</span><span class="n">k</span><span class="p">]):</span> 
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">phi</span><span class="o">**</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
           <span class="k">else</span><span class="p">:</span> <span class="c1"># If Armijo rule is fulfilled</span>
                <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

           <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
           <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">X_sample_grad</span><span class="p">,</span> <span class="n">Y_sample_grad</span><span class="p">,</span> <span class="o">*</span><span class="n">extra_args</span><span class="p">)</span>
           <span class="n">M1</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M2</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">M1</span> <span class="p">;</span> <span class="n">M3</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M4</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">@</span> <span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="p">;</span> <span class="n">M5</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">M1</span>
        
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">==</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">:</span>               
               <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">M1</span> <span class="o">@</span> <span class="n">M1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">M2</span> <span class="o">+</span> <span class="n">M3</span><span class="o">/</span><span class="n">M4</span>
           <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">rule</span> <span class="o">==</span> <span class="s1">&#39;SR1&#39;</span><span class="p">:</span>
               <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">M5</span><span class="nd">@M5</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">M5</span><span class="o">.</span><span class="n">T</span><span class="nd">@s</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
          
           <span class="n">break_iter</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
           <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective_change</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">objective_change</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">break_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">break_iter</span> <span class="o">=</span> <span class="n">break_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_values</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_values</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_optimal</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test how the algorithm works using some fixed parameters:</p>
<ul class="simple">
<li><p>Using BFGS rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                  <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\3752159407.py:74: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] - (M1 @ M1.T)/M2 + M3/M4
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 7.18562702,  7.1627397 ,  7.19182663, -0.01714026, -7.03894362,
       -3.56671191, -7.99991456,  3.57646817, -8.82325538,  8.00460544,
        6.27948023,  4.45873526,  0.86091811, -4.43487573,  1.81906301,
        1.8171754 , -0.86926416, -8.86679693,  4.48289918, -6.22418721,
       -8.90291632,  6.24586481,  5.4126446 , -0.73711489,  1.82152516,
        2.65890228])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1963.895407760215
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Using SR1 rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                  <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\3752159407.py:76: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] + (M5@M5.T) / (M5.T@s[k])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 7.18709518,  7.16422265,  7.1933678 , -0.0170576 , -7.04033306,
       -3.56738891, -8.00149054,  3.57725781, -8.82499498,  8.00626995,
        6.2807865 ,  4.45969831,  0.86115995, -4.43572224,  1.81953469,
        1.81758805, -0.86938913, -8.86858891,  4.48386123, -6.22540286,
       -8.90462897,  6.24717329,  5.41380763, -0.73721136,  1.82195469,
        2.65952481])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1964.0274917931274
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Stochastic_Quasi_Newton</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyper-parameters-optimization-hpo">
<h3><strong>Hyper-parameters Optimization (HPO)</strong><a class="headerlink" href="#hyper-parameters-optimization-hpo" title="Link to this heading">#</a></h3>
<p>We are going to apply HPO only on the most efficient methods (in terms of time):</p>
<ul class="simple">
<li><p>Gradient Descent</p></li>
<li><p>Approximated Gradient Descent (Mini-batch Stochastic Gradient Descent)</p></li>
<li><p>Coordinate Descent</p></li>
<li><p>Approximated Newton (Hessian-Free Inexact Newton )</p></li>
<li><p>Quasi Newton</p></li>
</ul>
<p>We discard Newton method due to its too expensive computational nature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomSearch</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_trials</span> <span class="o">=</span> <span class="n">n_trials</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        
        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>
        <span class="n">param_grid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span>

        <span class="n">combis</span><span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">param_grid</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="o">.</span><span class="n">keys</span><span class="p">()]))</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">random_combis</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">combis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trials</span><span class="p">)</span>
        <span class="n">random_combis</span> <span class="o">=</span> <span class="p">[{</span><span class="n">x</span><span class="p">:</span> <span class="n">random_combis</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_grid</span><span class="o">.</span><span class="n">keys</span><span class="p">())}</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">random_combis</span><span class="p">))]</span>

        <span class="n">objective_values</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span> <span class="p">;</span> <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_trials</span><span class="p">):</span>
            <span class="n">estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">random_combis</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">objective_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">)</span>
            <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="p">)</span>

        <span class="n">objective_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">objective_values</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_combis</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;objective_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">objective_values</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">times</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">results</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;params&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;objective_value&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">best_combi_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">objective_values</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_params_</span> <span class="o">=</span> <span class="n">random_combis</span><span class="p">[</span><span class="n">best_combi_idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_objective_value_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">objective_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<section id="hpo-for-gradient-descent">
<h4><strong>HPO for Gradient Descent</strong><a class="headerlink" href="#hpo-for-gradient-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">gradient_descent</span> <span class="o">=</span> <span class="n">gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">gradient_descent</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">gradient_descent_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">gradient_descent_HPO_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_descent_HPO_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.002442</td>
      <td>0.30</td>
      <td>0.026827</td>
      <td>1863.146834</td>
      <td>9.885241</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.002442</td>
      <td>0.28</td>
      <td>0.000295</td>
      <td>1863.146834</td>
      <td>11.479471</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.000373</td>
      <td>0.12</td>
      <td>0.000045</td>
      <td>1865.327773</td>
      <td>27.948953</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000256</td>
      <td>0.38</td>
      <td>0.000115</td>
      <td>1874.528566</td>
      <td>24.507261</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.009103</td>
      <td>0.42</td>
      <td>0.056899</td>
      <td>1877.511487</td>
      <td>1.397662</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.000176</td>
      <td>0.26</td>
      <td>0.003393</td>
      <td>1893.913114</td>
      <td>26.331573</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000146</td>
      <td>0.30</td>
      <td>0.000010</td>
      <td>1906.227519</td>
      <td>24.051028</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.071969</td>
      <td>0.14</td>
      <td>0.018421</td>
      <td>1925.525298</td>
      <td>23.601338</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.013257</td>
      <td>0.36</td>
      <td>0.012649</td>
      <td>1979.479904</td>
      <td>26.097165</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.086851</td>
      <td>0.44</td>
      <td>0.000015</td>
      <td>2126.844355</td>
      <td>2.998896</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hpo-for-stochastic-gradient-descent-mini-batch">
<h4><strong>HPO for Stochastic Gradient Descent (Mini-Batch)</strong><a class="headerlink" href="#hpo-for-stochastic-gradient-descent-mini-batch" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Without momentum</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">stochastic_gradient_descent</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                                                 <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
                                                                 <span class="n">momentum</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_gradient_descent</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">stochastic_gradient_descent_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_gradient_descent_HPO_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient_descent_HPO_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.022051</td>
      <td>0.40</td>
      <td>0.000095</td>
      <td>2020.486231</td>
      <td>0.782238</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.031993</td>
      <td>0.30</td>
      <td>0.000079</td>
      <td>2025.172304</td>
      <td>1.548264</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.002364</td>
      <td>0.46</td>
      <td>0.010481</td>
      <td>2025.596793</td>
      <td>0.835933</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.029151</td>
      <td>0.30</td>
      <td>0.000079</td>
      <td>2025.677830</td>
      <td>1.584038</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.009545</td>
      <td>0.36</td>
      <td>0.026827</td>
      <td>2026.601080</td>
      <td>1.253199</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.003765</td>
      <td>0.48</td>
      <td>0.015264</td>
      <td>2026.678324</td>
      <td>0.163731</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.005462</td>
      <td>0.40</td>
      <td>0.000012</td>
      <td>2026.906665</td>
      <td>0.182777</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient_descent_HPO_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24</th>
      <td>0.141747</td>
      <td>0.22</td>
      <td>0.001931</td>
      <td>2140.304183</td>
      <td>0.891123</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.067342</td>
      <td>0.20</td>
      <td>0.003393</td>
      <td>2141.884947</td>
      <td>28.123716</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.107227</td>
      <td>0.16</td>
      <td>0.000037</td>
      <td>2800.380652</td>
      <td>24.778555</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.327455</td>
      <td>0.26</td>
      <td>0.032375</td>
      <td>5024.030566</td>
      <td>0.430829</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.475081</td>
      <td>0.48</td>
      <td>0.000518</td>
      <td>14423.745296</td>
      <td>0.797195</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.689261</td>
      <td>0.46</td>
      <td>0.100000</td>
      <td>21098.564483</td>
      <td>1.684405</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>With momentum</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="c1"># phi_grid = np.arange(0.1, 1, 0.02) This grid seems working better</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">gamma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="n">gamma_grid</span><span class="p">}</span>

<span class="n">stochastic_gradient_descent_momentum</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                                                 <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
                                                                 <span class="n">momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_gradient_descent_momentum</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">stochastic_gradient_descent_momentum_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_gradient_descent_momentum_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient_descent_momentum_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>gamma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>11</th>
      <td>0.024201</td>
      <td>0.48</td>
      <td>0.000754</td>
      <td>0.003365</td>
      <td>2018.289185</td>
      <td>0.438165</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.000231</td>
      <td>0.40</td>
      <td>0.100000</td>
      <td>0.016452</td>
      <td>2025.516666</td>
      <td>0.751965</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.000192</td>
      <td>0.42</td>
      <td>0.000168</td>
      <td>0.066730</td>
      <td>2026.148768</td>
      <td>0.950603</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.000159</td>
      <td>0.44</td>
      <td>0.000031</td>
      <td>0.204555</td>
      <td>2026.267380</td>
      <td>0.902047</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.000305</td>
      <td>0.10</td>
      <td>0.056899</td>
      <td>0.001595</td>
      <td>2026.268305</td>
      <td>0.565241</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.000278</td>
      <td>0.26</td>
      <td>0.000095</td>
      <td>0.073260</td>
      <td>2026.278474</td>
      <td>0.512966</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.006579</td>
      <td>0.30</td>
      <td>0.100000</td>
      <td>0.004453</td>
      <td>2026.646076</td>
      <td>0.227001</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_gradient_descent_momentum_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>gamma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>15</th>
      <td>0.689261</td>
      <td>0.22</td>
      <td>0.000021</td>
      <td>0.006469</td>
      <td>12597.935250</td>
      <td>1.348698</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.572237</td>
      <td>0.40</td>
      <td>0.007197</td>
      <td>0.001205</td>
      <td>15281.831241</td>
      <td>0.335829</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.911163</td>
      <td>0.20</td>
      <td>0.000754</td>
      <td>0.002543</td>
      <td>15491.691129</td>
      <td>1.714045</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.830218</td>
      <td>0.42</td>
      <td>0.068665</td>
      <td>0.066730</td>
      <td>23518.583238</td>
      <td>0.195750</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.008697</td>
      <td>0.16</td>
      <td>0.000168</td>
      <td>1.000000</td>
      <td>538391.505433</td>
      <td>34.289111</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hpo-for-coordinate-descent">
<h4><strong>HPO for Coordinate Descent</strong><a class="headerlink" href="#hpo-for-coordinate-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">coordinate_descent</span> <span class="o">=</span> <span class="n">coordinate_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">derivative</span><span class="o">=</span><span class="n">deriv_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span>
                                               <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">coordinate_descent</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">coordinate_descent_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">coordinate_descent_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_descent_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22</th>
      <td>0.138950</td>
      <td>0.32</td>
      <td>0.022230</td>
      <td>1996.921724</td>
      <td>5.725464</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.568987</td>
      <td>0.14</td>
      <td>0.001600</td>
      <td>2001.795061</td>
      <td>4.596580</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.323746</td>
      <td>0.10</td>
      <td>0.000168</td>
      <td>2005.609900</td>
      <td>13.366650</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.152642</td>
      <td>0.24</td>
      <td>0.003393</td>
      <td>2009.291799</td>
      <td>3.495496</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.390694</td>
      <td>0.16</td>
      <td>0.000139</td>
      <td>2010.296835</td>
      <td>2.718234</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.014563</td>
      <td>0.32</td>
      <td>0.002330</td>
      <td>2010.400128</td>
      <td>2.658906</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.021210</td>
      <td>0.44</td>
      <td>0.000012</td>
      <td>2010.466983</td>
      <td>5.249625</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coordinate_descent_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>72</th>
      <td>1.000000</td>
      <td>0.26</td>
      <td>0.001099</td>
      <td>254442.926676</td>
      <td>0.233699</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.568987</td>
      <td>0.38</td>
      <td>0.000518</td>
      <td>255399.075807</td>
      <td>0.285439</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.568987</td>
      <td>0.42</td>
      <td>0.000045</td>
      <td>273047.732109</td>
      <td>0.350267</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.828643</td>
      <td>0.32</td>
      <td>0.000244</td>
      <td>295515.079534</td>
      <td>0.232251</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.625055</td>
      <td>0.44</td>
      <td>0.003393</td>
      <td>316187.094735</td>
      <td>0.402382</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.828643</td>
      <td>0.48</td>
      <td>0.000910</td>
      <td>342765.122511</td>
      <td>0.385080</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.686649</td>
      <td>0.48</td>
      <td>0.007197</td>
      <td>345851.793712</td>
      <td>0.224334</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hpo-for-stochastic-newton">
<h4><strong>HPO for Stochastic Newton</strong><a class="headerlink" href="#hpo-for-stochastic-newton" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">stochastic_newton</span> <span class="o">=</span> <span class="n">Stochastic_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                          <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_newton</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">stochastic_newton_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_newton_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_newton_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.148497</td>
      <td>0.40</td>
      <td>0.000095</td>
      <td>1876.491136</td>
      <td>0.975895</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.259502</td>
      <td>0.20</td>
      <td>0.003393</td>
      <td>1877.342791</td>
      <td>0.508629</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.170735</td>
      <td>0.30</td>
      <td>0.000079</td>
      <td>1878.328677</td>
      <td>3.272935</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.178865</td>
      <td>0.30</td>
      <td>0.000079</td>
      <td>1878.646623</td>
      <td>3.585913</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.327455</td>
      <td>0.16</td>
      <td>0.000037</td>
      <td>1883.894920</td>
      <td>0.941932</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.187382</td>
      <td>0.22</td>
      <td>0.000015</td>
      <td>1884.010046</td>
      <td>3.466109</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.376494</td>
      <td>0.22</td>
      <td>0.001931</td>
      <td>1885.866089</td>
      <td>0.466623</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.061359</td>
      <td>0.48</td>
      <td>0.015264</td>
      <td>1888.792591</td>
      <td>3.293207</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.073907</td>
      <td>0.40</td>
      <td>0.000012</td>
      <td>1894.109505</td>
      <td>1.699789</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.048626</td>
      <td>0.46</td>
      <td>0.010481</td>
      <td>1894.966149</td>
      <td>2.067125</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_newton_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>26</th>
      <td>0.015199</td>
      <td>0.16</td>
      <td>0.000045</td>
      <td>1957.461611</td>
      <td>20.699191</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.012619</td>
      <td>0.10</td>
      <td>0.000010</td>
      <td>1967.396674</td>
      <td>13.550175</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.010000</td>
      <td>0.18</td>
      <td>0.000295</td>
      <td>1978.539699</td>
      <td>21.549985</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.689261</td>
      <td>0.48</td>
      <td>0.000518</td>
      <td>232031.499821</td>
      <td>0.183833</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.830218</td>
      <td>0.46</td>
      <td>0.100000</td>
      <td>927875.724057</td>
      <td>0.142736</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hpo-for-quasi-newton">
<h4><strong>HPO for Quasi Newton</strong><a class="headerlink" href="#hpo-for-quasi-newton" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>With BFGS rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">quasi_newton</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                          <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">quasi_newton</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">quasi_newton_BFGS_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">quasi_newton_BFGS_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\4251161217.py:52: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] - (M1 @ M1.T)/M2 + M3/M4
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quasi_newton_BFGS_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>18</th>
      <td>0.686649</td>
      <td>0.48</td>
      <td>0.007197</td>
      <td>1863.508551</td>
      <td>3.016087</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.390694</td>
      <td>0.16</td>
      <td>0.000139</td>
      <td>1863.631774</td>
      <td>52.651179</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.294705</td>
      <td>0.44</td>
      <td>0.000015</td>
      <td>1867.310267</td>
      <td>13.962198</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.294705</td>
      <td>0.40</td>
      <td>0.000115</td>
      <td>1867.310467</td>
      <td>19.225129</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.268270</td>
      <td>0.14</td>
      <td>0.018421</td>
      <td>1867.460748</td>
      <td>2.753597</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.138950</td>
      <td>0.32</td>
      <td>0.022230</td>
      <td>1882.469259</td>
      <td>3.400620</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.115140</td>
      <td>0.36</td>
      <td>0.012649</td>
      <td>1888.265757</td>
      <td>3.374103</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quasi_newton_BFGS_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>0.015999</td>
      <td>0.38</td>
      <td>0.000115</td>
      <td>1925.849287</td>
      <td>10.142552</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.012068</td>
      <td>0.30</td>
      <td>0.000010</td>
      <td>1930.049585</td>
      <td>12.845520</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.010000</td>
      <td>0.14</td>
      <td>0.000054</td>
      <td>1931.613712</td>
      <td>15.124855</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.013257</td>
      <td>0.26</td>
      <td>0.003393</td>
      <td>1933.858520</td>
      <td>4.198291</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.013257</td>
      <td>0.26</td>
      <td>0.000012</td>
      <td>1933.858520</td>
      <td>4.354548</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>With SR1 rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">quasi_newton</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                          <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">quasi_newton</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">quasi_newton_SR1_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">quasi_newton_SR1_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quasi_newton_SR1_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>18</th>
      <td>0.686649</td>
      <td>0.48</td>
      <td>0.007197</td>
      <td>1863.148509</td>
      <td>2.851664</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.390694</td>
      <td>0.16</td>
      <td>0.000139</td>
      <td>1863.857186</td>
      <td>2.599789</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.294705</td>
      <td>0.40</td>
      <td>0.000115</td>
      <td>1867.167745</td>
      <td>2.763651</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.294705</td>
      <td>0.44</td>
      <td>0.000015</td>
      <td>1867.167745</td>
      <td>2.932500</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.268270</td>
      <td>0.14</td>
      <td>0.018421</td>
      <td>1868.903042</td>
      <td>2.783462</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.138950</td>
      <td>0.32</td>
      <td>0.022230</td>
      <td>1886.624232</td>
      <td>3.302841</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.115140</td>
      <td>0.36</td>
      <td>0.012649</td>
      <td>1892.318794</td>
      <td>3.434315</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quasi_newton_SR1_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>0.015999</td>
      <td>0.38</td>
      <td>0.000115</td>
      <td>1930.439228</td>
      <td>10.289297</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.013257</td>
      <td>0.26</td>
      <td>0.003393</td>
      <td>1932.061576</td>
      <td>11.571738</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.013257</td>
      <td>0.26</td>
      <td>0.000012</td>
      <td>1932.061576</td>
      <td>11.984692</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.012068</td>
      <td>0.30</td>
      <td>0.000010</td>
      <td>1932.787078</td>
      <td>12.738293</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.010000</td>
      <td>0.14</td>
      <td>0.000054</td>
      <td>1934.084561</td>
      <td>15.064416</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hpo-for-stochastic-quasi-newton">
<h4><strong>HPO for Stochastic Quasi Newton</strong><a class="headerlink" href="#hpo-for-stochastic-quasi-newton" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>With BFGS rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">stochastic_quasi_newton</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                          <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_quasi_newton</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">stochastic_quasi_newton_BFGS_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_quasi_newton_BFGS_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\3752159407.py:74: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] - (M1 @ M1.T)/M2 + M3/M4
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>43</th>
      <td>0.167683</td>
      <td>0.40</td>
      <td>0.000026</td>
      <td>1872.936608</td>
      <td>0.362295</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.184207</td>
      <td>0.46</td>
      <td>0.000054</td>
      <td>1875.414095</td>
      <td>0.776982</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.184207</td>
      <td>0.44</td>
      <td>0.000054</td>
      <td>1878.502318</td>
      <td>0.597597</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.138950</td>
      <td>0.32</td>
      <td>0.022230</td>
      <td>1882.293136</td>
      <td>0.444986</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.138950</td>
      <td>0.18</td>
      <td>0.000625</td>
      <td>1883.549541</td>
      <td>0.681427</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.222300</td>
      <td>0.34</td>
      <td>0.000018</td>
      <td>1884.952042</td>
      <td>1.663121</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.138950</td>
      <td>0.28</td>
      <td>0.000244</td>
      <td>1886.246763</td>
      <td>0.480609</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>60</th>
      <td>0.625055</td>
      <td>0.44</td>
      <td>0.003393</td>
      <td>34513.990948</td>
      <td>1.797356</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.686649</td>
      <td>0.48</td>
      <td>0.007197</td>
      <td>34840.604570</td>
      <td>2.106132</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.828643</td>
      <td>0.48</td>
      <td>0.000910</td>
      <td>36298.218661</td>
      <td>1.416631</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.828643</td>
      <td>0.32</td>
      <td>0.000244</td>
      <td>37982.610337</td>
      <td>0.709809</td>
    </tr>
    <tr>
      <th>72</th>
      <td>1.000000</td>
      <td>0.26</td>
      <td>0.001099</td>
      <td>39684.618104</td>
      <td>0.416479</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>With SR1 rule</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">sigma_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma_grid</span><span class="p">}</span>

<span class="n">stochastic_quasi_newton</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                          <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_quasi_newton</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">stochastic_quasi_newton_SR1_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_quasi_newton_SR1_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\3752159407.py:76: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] + (M5@M5.T) / (M5.T@s[k])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_SR1_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>0.162975</td>
      <td>0.38</td>
      <td>0.000021</td>
      <td>1874.854659</td>
      <td>0.583116</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.247708</td>
      <td>0.42</td>
      <td>0.000066</td>
      <td>1877.042685</td>
      <td>1.597495</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.132194</td>
      <td>0.20</td>
      <td>0.003393</td>
      <td>1882.032230</td>
      <td>0.994149</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.114976</td>
      <td>0.18</td>
      <td>0.010481</td>
      <td>1883.494585</td>
      <td>1.335751</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.132194</td>
      <td>0.32</td>
      <td>0.005964</td>
      <td>1884.667031</td>
      <td>0.482332</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.114976</td>
      <td>0.24</td>
      <td>0.001326</td>
      <td>1885.380768</td>
      <td>0.857913</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.107227</td>
      <td>0.18</td>
      <td>0.000037</td>
      <td>1887.017355</td>
      <td>1.384988</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_SR1_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>sigma</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>75</th>
      <td>0.756463</td>
      <td>0.14</td>
      <td>0.007197</td>
      <td>2201.283558</td>
      <td>5.932433</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.432876</td>
      <td>0.26</td>
      <td>0.032375</td>
      <td>2233.781590</td>
      <td>39.886768</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.756463</td>
      <td>0.46</td>
      <td>0.100000</td>
      <td>3431.731467</td>
      <td>5.131155</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.572237</td>
      <td>0.48</td>
      <td>0.000518</td>
      <td>3523.595415</td>
      <td>2.851810</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.932603</td>
      <td>0.44</td>
      <td>0.000010</td>
      <td>38552.783918</td>
      <td>0.669038</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we define the models we will use throughout  the following section. These models will have set the hyper-parameters obtained in the HPO phase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">########################################################################################################################</span>

<span class="n">gradient_descent</span> <span class="o">=</span> <span class="n">gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> 
                                           <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">gradient_descent</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">gradient_descent_best_params</span><span class="p">)</span>
<span class="n">gradient_descent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">coordinate_descent</span> <span class="o">=</span> <span class="n">coordinate_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">derivative</span><span class="o">=</span><span class="n">deriv_function</span><span class="p">,</span> 
                                              <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="n">coordinate_descent</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">coordinate_descent_best_params</span><span class="p">)</span>
<span class="n">coordinate_descent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">stochastic_gradient_descent</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> 
                                                         <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                                         <span class="n">sample_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_gradient_descent_best_params</span><span class="p">)</span>
<span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">stochastic_gradient_descent_momentum</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> 
                                                                  <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                                                  <span class="n">sample_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_gradient_descent_momentum_best_params</span><span class="p">)</span>
<span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">newton</span> <span class="o">=</span> <span class="n">Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span>
                                                         <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                                         <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">stochastic_newton</span> <span class="o">=</span> <span class="n">Stochastic_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span>
                                             <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                             <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">stochastic_newton</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_newton_best_params</span><span class="p">)</span>
<span class="n">stochastic_newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>


<span class="c1">########################################################################################################################</span>

<span class="n">quasi_newton_BFGS</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span>
                                        <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                        <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
<span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">quasi_newton_BFGS_best_params</span><span class="p">)</span>
<span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>


<span class="c1">########################################################################################################################</span>

<span class="n">quasi_newton_SR1</span> <span class="o">=</span> <span class="n">Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span>
                                        <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
                                        <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">)</span>
<span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">quasi_newton_SR1_best_params</span><span class="p">)</span>
<span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>



<span class="c1">########################################################################################################################</span>

<span class="n">stochastic_quasi_newton_BFGS</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                                         <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> 
                                                         <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_quasi_newton_BFGS_best_params</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>

<span class="n">stochastic_quasi_newton_SR1</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                                         <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;SR1&#39;</span><span class="p">,</span> 
                                                         <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_quasi_newton_SR1_best_params</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1">########################################################################################################################</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="analysis-of-convergence">
<h3><strong>Analysis of convergence</strong><a class="headerlink" href="#analysis-of-convergence" title="Link to this heading">#</a></h3>
<p>In this section we are going to analyze how each one of the implemented models converge.</p>
<section id="convergence-of-gradient-descent">
<h4><strong>Convergence of Gradient Descent</strong><a class="headerlink" href="#convergence-of-gradient-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient Descent - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient Descent run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient Descent optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient Descent iterations:&#39;</span><span class="p">,</span> <span class="n">gradient_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">gradient_descent</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6426bfc0f5a385cd9a88543dc5f760b90e9cd8f421a2bcf1250b90c21df9c8bd.png" src="_images/6426bfc0f5a385cd9a88543dc5f760b90e9cd8f421a2bcf1250b90c21df9c8bd.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient Descent run time: 9.6
Gradient Descent optimal objective: 1863.15
Gradient Descent iterations: 847
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 0.0
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Gradient Descent implementation is not good in terms of iterations needed to converge, since it needs 847 iterations, what means 9.6 secs to converge, what is too much in comparison with the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs). But, in spite of that, the convergence value is basically the benchmark value (null divergence), so, in terms of approximating the optimal of the objective function is pretty good.</p>
</section>
<section id="convergence-of-coordinate-descent">
<h4><strong>Convergence of Coordinate Descent</strong><a class="headerlink" href="#convergence-of-coordinate-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start_iter</span> <span class="o">=</span> <span class="mi">240</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> 
             <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">coordinate_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:],</span> 
             <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">coordinate_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations[</span><span class="si">{</span><span class="n">start_iter</span><span class="si">}</span><span class="s1">:]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">coordinate_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">coordinate_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">y_ticks_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">]</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">]]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                                                                                                                <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">y_ticks_0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:]),</span> 
                                                  <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:]),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Coordinate Descent - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coordinate Descent run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coordinate Descent optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coordinate Descent iterations:&#39;</span><span class="p">,</span> <span class="n">coordinate_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">coordinate_descent</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1b4d7192ed34afa839cf3ab3463a2ad7d9db04a181d37924832d4428ec823d31.png" src="_images/1b4d7192ed34afa839cf3ab3463a2ad7d9db04a181d37924832d4428ec823d31.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coordinate Descent run time: 5.55
Coordinate Descent optimal objective: 1996.92
Coordinate Descent iterations: 490
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 133.77
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Coordinate Descent implementation is not good in terms of iterations needed to converge, since it needs 490 iterations, what means 5.55 secs to converge, what is too much in comparison with the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but less than with Gradient Descent.</p>
<p>Besides, the convergence value is a bit far from the benchmark value (divergence of 133.77), so, in terms of approximating the optimal of the objective function is not as good as Gradient Descent.</p>
</section>
<section id="convergence-of-stochastic-gradient-descent">
<h4><strong>Convergence of Stochastic Gradient Descent</strong><a class="headerlink" href="#convergence-of-stochastic-gradient-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                             <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="c1">#ax.axhline(y=benchmark_results[&#39;sklearn&#39;][&#39;fun&#39;], color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;Sklearn Benchmark&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent (Mini-Batch) - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Descent optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/821628f751c8fd8a5cf69fc0a851326863fdf939b90ce4b71c68de339eebba80.png" src="_images/821628f751c8fd8a5cf69fc0a851326863fdf939b90ce4b71c68de339eebba80.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stochastic Gradient Descent run time: 0.76
Stochastic Descent optimal objective: 2020.49
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 157.34
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Stochastic Gradient Descent (Mini-Batch) implementation is very good in terms of iterations needed to converge, since it needs 64 iterations, what means 0.76 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not much more, and quite less than with the previous methods.</p>
<p>Besides, the convergence value is a bit far from the benchmark value (divergence of 157.34), so, in terms of approximating the optimal of the objective function is not as good as above methods.</p>
</section>
<section id="convergence-of-stochastic-gradient-descent-with-momentum">
<h4><strong>Convergence of Stochastic Gradient Descent with Momentum</strong><a class="headerlink" href="#convergence-of-stochastic-gradient-descent-with-momentum" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                             <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="c1">#ax.axhline(y=benchmark_results[&#39;sklearn&#39;][&#39;fun&#39;], color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;Sklearn Benchmark&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent (Mini-Batch) with Momentum - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent (momentum) run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Descent (momentum) optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_gradient_descent_momentum</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/feb271e74f09150e906725b29a1d66154564f82f9824a971d5b4a750e8ddf9d3.png" src="_images/feb271e74f09150e906725b29a1d66154564f82f9824a971d5b4a750e8ddf9d3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stochastic Gradient Descent (momentum) run time: 0.42
Stochastic Descent (momentum) optimal objective: 2018.29
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 155.14
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Stochastic Gradient Descent (Mini-Batch) with momentum implementation is good in terms of iterations needed to converge, since it just needs 35 iterations, what means 0.42 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not much more, and less than with the previous methods.</p>
<p>Besides, the convergence value is a bit far from the benchmark value (divergence of 155.14), so, in terms of approximating the optimal of the objective function is better than Stochastic Gradient Descent without momentum.</p>
</section>
<section id="convergence-of-newton-s-method">
<h4><strong>Convergence of Newton’s Method</strong><a class="headerlink" href="#convergence-of-newton-s-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Newton</span><span class="se">\&#39;</span><span class="s1">s Method -  Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Newton</span><span class="se">\&#39;</span><span class="s1">s Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">newton</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Newton</span><span class="se">\&#39;</span><span class="s1">s Method optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">newton</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">newton</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/932d7f8a74147f9548dfd601b739db73c217e9a8d9c3b570b295174c6f0677d2.png" src="_images/932d7f8a74147f9548dfd601b739db73c217e9a8d9c3b570b295174c6f0677d2.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Newton&#39;s Method run time: 193.38
Newton&#39;s Method optimal objective: 1863.15
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 0.0
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Newton’s Method implementation is bad in terms of iterations needed to converge, since it needs 80 iterations, what means 193.38 secs to converge, what is much more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), and it’s by far the most expensive method.</p>
<p>But, in spite of that, the convergence value is the same of the benchmark value (null divergence), so, in terms of approximating the optimal of the objective function is brilliant.</p>
</section>
<section id="convergence-of-stochastic-newton-s-method">
<h4><strong>Convergence of Stochastic Newton’s Method</strong><a class="headerlink" href="#convergence-of-stochastic-newton-s-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start_iter</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> 
             <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:],</span> 
             <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations[</span><span class="si">{</span><span class="n">start_iter</span><span class="si">}</span><span class="s1">:]&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">y_ticks_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                                                                                  <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">y_ticks_0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:]),</span> 
                                      <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_values</span><span class="p">[</span><span class="n">start_iter</span><span class="p">:]),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Stochastic Newton</span><span class="se">\&#39;</span><span class="s1">s Method - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Newton</span><span class="se">\&#39;</span><span class="s1">s Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Newton</span><span class="se">\&#39;</span><span class="s1">s Method optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/33c9d4b6aef97cd35510114fa63d44194efdf9b23c6efa62bde93a67ce256b95.png" src="_images/33c9d4b6aef97cd35510114fa63d44194efdf9b23c6efa62bde93a67ce256b95.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stochastic Newton&#39;s Method run time: 0.84
Stochastic Newton&#39;s Method optimal objective: 1876.49
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 13.34
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Stochastic Newton’s Method implementation is good in terms of iterations needed to converge, since it needs 41 iterations, what means 0.84 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not much.</p>
<p>In addition, the convergence value is close to the benchmark value (13.34 divergence), so, in terms of approximating the optimal of the objective function is pretty good.</p>
<p>This is so far the best of our Stochastic implementations.</p>
</section>
<section id="convergence-of-quasi-newton-s-bfgs-method">
<h4><strong>Convergence of Quasi-Newton’s BFGS Method</strong><a class="headerlink" href="#convergence-of-quasi-newton-s-bfgs-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s Method BFGS - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS  optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0e0e425be8fe36d145386290003b22b7fc6ea366e55b5d5a1cb8fc13a29e2048.png" src="_images/0e0e425be8fe36d145386290003b22b7fc6ea366e55b5d5a1cb8fc13a29e2048.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Quasi-Newton&#39;s BFGS Method run time: 3.06
Quasi-Newton&#39;s BFGS  optimal objective: 1863.51
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 0.36
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Quasi-Newton’s BFGS Method implementation is good in terms of iterations needed to converge, since it needs 25 iterations, what means 3.06 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not too much.</p>
<p>In addition, the convergence value is practically the same of the benchmark value (0.36 divergence), so, in terms of approximating the optimal of the objective function is pretty good.</p>
</section>
<section id="convergence-of-quasi-newton-s-sr1-method">
<h4><strong>Convergence of Quasi-Newton’s SR1 Method</strong><a class="headerlink" href="#convergence-of-quasi-newton-s-sr1-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Quasi Newton</span><span class="se">\&#39;</span><span class="s1">s SR1 Method - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s SR1 Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s SR1  optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9168618d69fdd9c751eb66c9885b639479eb697432ef4bbc7fbec1427a56908e.png" src="_images/9168618d69fdd9c751eb66c9885b639479eb697432ef4bbc7fbec1427a56908e.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Quasi-Newton&#39;s SR1 Method run time: 3.03
Quasi-Newton&#39;s SR1  optimal objective: 1863.15
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 0.0
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Quasi-Newton’s SR1 Method implementation is good in terms of iterations needed to converge, since it needs 25 iterations, what means 3.03 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not too much.</p>
<p>In addition, the convergence value is the same of the benchmark value (null divergence), so, in terms of approximating the optimal of the objective function is perfect.</p>
</section>
<section id="stochastic-convergence-of-quasi-newton-s-bfgs-method">
<h4><strong>Stochastic Convergence of Quasi-Newton’s BFGS Method</strong><a class="headerlink" href="#stochastic-convergence-of-quasi-newton-s-bfgs-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                                                                                    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">)))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS Method - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS  optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/65fe4a37013b16945f8735531a112e48c050054070dec2c572f9b2f12e0ff04e.png" src="_images/65fe4a37013b16945f8735531a112e48c050054070dec2c572f9b2f12e0ff04e.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stochastic Quasi-Newton&#39;s BFGS Method run time: 0.54
Stochastic Quasi-Newton&#39;s BFGS  optimal objective: 1872.94
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 9.79
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Stochastic Quasi-Newton’s BFGS Method implementation is pretty good in terms of iterations needed to converge, since it needs 23 iterations, what means 0.54 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but not much.</p>
<p>In addition, the convergence value is the close to the benchmark value (9.79 divergence), so, in terms of approximating the optimal of the objective function is good.</p>
</section>
<section id="stochastic-convergence-of-quasi-newton-s-sr1-method">
<h4><strong>Stochastic Convergence of Quasi-Newton’s SR1 Method</strong><a class="headerlink" href="#stochastic-convergence-of-quasi-newton-s-sr1-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">break_iter</span><span class="p">),</span> 
             <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Objective Function vs Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Objective values&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">break_iter</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> 
                                                                                                    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_values</span><span class="p">),</span> <span class="mi">10</span><span class="p">)))))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sklearn Benchmark&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi Newton</span><span class="se">\&#39;</span><span class="s1">s SR1 Method - Convergence&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s SR1 Method run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s SR1  optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn run time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error (divergence):&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_SR1</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="o">-</span> <span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45d3dd58627a1a996d0b3404ac25e199fe59d1f9db1258a77377e260da260993.png" src="_images/45d3dd58627a1a996d0b3404ac25e199fe59d1f9db1258a77377e260da260993.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stochastic Quasi-Newton&#39;s SR1 Method run time: 0.34
Stochastic Quasi-Newton&#39;s SR1  optimal objective: 1874.85
Sklearn run time: 0.04
Sklearn optimal objective: 1863.15
Error (divergence): 11.71
</pre></div>
</div>
</div>
</div>
<p>The convergence of our Stochastic Quasi-Newton’s SR1 Method implementation is pretty good in terms of iterations needed to converge, since it needs 16 iterations, what means 0.34 secs to converge, what is more than <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> benchmark (0.04 secs), but a bit less than with BFGS rule.</p>
<p>In addition, the convergence value is the close to the benchmark value (11.71 divergence), so, in terms of approximating the optimal of the objective function is good but a bit more than with the BFGS rule.</p>
</section>
</section>
<section id="methods-comparison-optimal-values-vs-times">
<h3><strong>Methods Comparison: Optimal values vs Times</strong><a class="headerlink" href="#methods-comparison-optimal-values-vs-times" title="Link to this heading">#</a></h3>
<p>In this section we are going to compare all the tested methods (both our implementations and the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">scipy</span></code> ones) in terms of optimality and time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">methods</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient_descent</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;coordinate_descent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">coordinate_descent</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;stochastic_gradient_descent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;stochastic_gradient_descent_momentum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_momentum</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">newton</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;stochastic_newton&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stochastic_newton</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;quasi_newton_BFGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">quasi_newton_BFGS</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;quasi_newton_SR1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">quasi_newton_SR1</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;stochastic_quasi_newton_BFGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stochastic_quasi_newton_BFGS</span>
<span class="n">methods</span><span class="p">[</span><span class="s1">&#39;stochastic_quasi_newton_SR1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stochastic_quasi_newton_SR1</span>

<span class="n">methods_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">methods</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">optimal_values_methods</span> <span class="o">=</span> <span class="p">[</span><span class="n">methods</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">objective_optimal</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">methods_list</span><span class="p">]</span>
<span class="n">times_methods</span> <span class="o">=</span> <span class="p">[</span><span class="n">methods</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">run_time</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">methods_list</span><span class="p">]</span>

<span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">benchmark_methods</span><span class="p">:</span>
    <span class="n">methods_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="n">optimal_values_methods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">])</span>
    <span class="n">times_methods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">])</span>

<span class="n">best_method_objective</span> <span class="o">=</span> <span class="n">methods_list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">best_objective</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="p">)</span>
<span class="n">best_method_time</span> <span class="o">=</span> <span class="n">methods_list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">times_methods</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">best_time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">times_methods</span><span class="p">)</span>
<span class="n">optimal_values_methods</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="p">)</span>
<span class="n">times_methods</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">times_methods</span><span class="p">)</span>
<span class="n">error_to_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">optimal_values_methods</span> <span class="o">-</span> <span class="n">best_objective</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">time_to_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">times_methods</span> <span class="o">-</span> <span class="n">best_time</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">comparison_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span> <span class="n">methods_list</span><span class="p">,</span> <span class="s1">&#39;Optimality&#39;</span><span class="p">:</span> <span class="n">optimal_values_methods</span><span class="p">,</span> <span class="s1">&#39;Error&#39;</span><span class="p">:</span> <span class="n">error_to_best</span><span class="p">,</span> 
                                 <span class="s1">&#39;Time&#39;</span><span class="p">:</span> <span class="n">times_methods</span><span class="p">,</span> <span class="s1">&#39;Time_to_best&#39;</span><span class="p">:</span> <span class="n">time_to_best</span><span class="p">})</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">times_methods</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">times_methods_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">times_methods</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">optimal_values_methods_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">best_method_objective_scaled</span> <span class="o">=</span> <span class="n">methods_list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">optimal_values_methods_scaled</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">best_objective_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">optimal_values_methods_scaled</span><span class="p">)</span>
<span class="n">best_method_time_scaled</span> <span class="o">=</span> <span class="n">methods_list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">times_methods_scaled</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">best_time_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">times_methods_scaled</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">optimal_values_methods</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">methods_list</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">best_objective</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_method_objective</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">times_methods</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">methods_list</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">best_time</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_method_time</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Methods vs Objective Function&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Methods vs Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Methods&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="c1">#axes[1].set_ylabel(&#39;Objective values&#39;, fontsize=12)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">optimal_values_methods</span><span class="p">),</span> <span class="mi">7</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">times_methods</span><span class="p">)</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Methods Comparison - Optimality and Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/47ab8c2e7bad441a76414ed0caa762a3a1f1a861fb5c8bafb3b1f8e6658bc1a5.png" src="_images/47ab8c2e7bad441a76414ed0caa762a3a1f1a861fb5c8bafb3b1f8e6658bc1a5.png" />
</div>
</div>
<p>In order to make a more fruitful comparison we are going to scale the objective and time values, to bring them to the same scale and make them comparable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">optimal_values_methods_scaled</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">methods_list</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Objective value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">best_objective_scaled</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_method_objective_scaled</span><span class="p">],</span> 
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">times_methods_scaled</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">methods_list</span><span class="p">,</span> 
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">best_time_scaled</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">best_method_time_scaled</span><span class="p">],</span> 
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Methods Comparison - Optimality vs Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Objective - Time (MinMax Scaled)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Methods&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.42</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS Method time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">run_time</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stochastic Quasi-Newton</span><span class="se">\&#39;</span><span class="s1">s BFGS  optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn time:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sklearn optimal objective:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;sklearn&#39;</span><span class="p">][</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6a51dcbc805c8c35340d08ac9c65b9b6e3fdf74c83ca7b1ed82965e43aaf007f.png" src="_images/6a51dcbc805c8c35340d08ac9c65b9b6e3fdf74c83ca7b1ed82965e43aaf007f.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">comparison_table</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Optimality</th>
      <th>Error</th>
      <th>Time</th>
      <th>Time_to_best</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>gradient_descent</td>
      <td>1863.146834</td>
      <td>0.00</td>
      <td>9.599391</td>
      <td>9.56</td>
    </tr>
    <tr>
      <th>4</th>
      <td>newton</td>
      <td>1863.146789</td>
      <td>0.00</td>
      <td>209.193209</td>
      <td>209.15</td>
    </tr>
    <tr>
      <th>7</th>
      <td>quasi_newton_SR1</td>
      <td>1863.148509</td>
      <td>0.00</td>
      <td>3.032212</td>
      <td>2.99</td>
    </tr>
    <tr>
      <th>10</th>
      <td>sklearn</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>0.044000</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>11</th>
      <td>scipy_BFGS</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>4.999240</td>
      <td>4.96</td>
    </tr>
    <tr>
      <th>12</th>
      <td>scipy_Newton-CG</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>19.659599</td>
      <td>19.62</td>
    </tr>
    <tr>
      <th>13</th>
      <td>scipy_L-BFGS-B</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>0.983402</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>14</th>
      <td>scipy_SLSQP</td>
      <td>1863.146796</td>
      <td>0.00</td>
      <td>1.614245</td>
      <td>1.57</td>
    </tr>
    <tr>
      <th>6</th>
      <td>quasi_newton_BFGS</td>
      <td>1863.508551</td>
      <td>0.36</td>
      <td>3.060085</td>
      <td>3.02</td>
    </tr>
    <tr>
      <th>8</th>
      <td>stochastic_quasi_newton_BFGS</td>
      <td>1872.936608</td>
      <td>9.79</td>
      <td>0.536080</td>
      <td>0.49</td>
    </tr>
    <tr>
      <th>9</th>
      <td>stochastic_quasi_newton_SR1</td>
      <td>1874.854659</td>
      <td>11.71</td>
      <td>0.341867</td>
      <td>0.30</td>
    </tr>
    <tr>
      <th>5</th>
      <td>stochastic_newton</td>
      <td>1876.491136</td>
      <td>13.34</td>
      <td>0.837814</td>
      <td>0.79</td>
    </tr>
    <tr>
      <th>1</th>
      <td>coordinate_descent</td>
      <td>1996.921724</td>
      <td>133.77</td>
      <td>5.552428</td>
      <td>5.51</td>
    </tr>
    <tr>
      <th>3</th>
      <td>stochastic_gradient_descent_momentum</td>
      <td>2018.289185</td>
      <td>155.14</td>
      <td>0.416592</td>
      <td>0.37</td>
    </tr>
    <tr>
      <th>2</th>
      <td>stochastic_gradient_descent</td>
      <td>2020.486231</td>
      <td>157.34</td>
      <td>0.758967</td>
      <td>0.71</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">comparison_table</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Optimality</th>
      <th>Error</th>
      <th>Time</th>
      <th>Time_to_best</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>10</th>
      <td>sklearn</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>0.044000</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>9</th>
      <td>stochastic_quasi_newton_SR1</td>
      <td>1874.854659</td>
      <td>11.71</td>
      <td>0.341867</td>
      <td>0.30</td>
    </tr>
    <tr>
      <th>3</th>
      <td>stochastic_gradient_descent_momentum</td>
      <td>2018.289185</td>
      <td>155.14</td>
      <td>0.416592</td>
      <td>0.37</td>
    </tr>
    <tr>
      <th>8</th>
      <td>stochastic_quasi_newton_BFGS</td>
      <td>1872.936608</td>
      <td>9.79</td>
      <td>0.536080</td>
      <td>0.49</td>
    </tr>
    <tr>
      <th>2</th>
      <td>stochastic_gradient_descent</td>
      <td>2020.486231</td>
      <td>157.34</td>
      <td>0.758967</td>
      <td>0.71</td>
    </tr>
    <tr>
      <th>5</th>
      <td>stochastic_newton</td>
      <td>1876.491136</td>
      <td>13.34</td>
      <td>0.837814</td>
      <td>0.79</td>
    </tr>
    <tr>
      <th>13</th>
      <td>scipy_L-BFGS-B</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>0.983402</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>14</th>
      <td>scipy_SLSQP</td>
      <td>1863.146796</td>
      <td>0.00</td>
      <td>1.614245</td>
      <td>1.57</td>
    </tr>
    <tr>
      <th>7</th>
      <td>quasi_newton_SR1</td>
      <td>1863.148509</td>
      <td>0.00</td>
      <td>3.032212</td>
      <td>2.99</td>
    </tr>
    <tr>
      <th>6</th>
      <td>quasi_newton_BFGS</td>
      <td>1863.508551</td>
      <td>0.36</td>
      <td>3.060085</td>
      <td>3.02</td>
    </tr>
    <tr>
      <th>11</th>
      <td>scipy_BFGS</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>4.999240</td>
      <td>4.96</td>
    </tr>
    <tr>
      <th>1</th>
      <td>coordinate_descent</td>
      <td>1996.921724</td>
      <td>133.77</td>
      <td>5.552428</td>
      <td>5.51</td>
    </tr>
    <tr>
      <th>0</th>
      <td>gradient_descent</td>
      <td>1863.146834</td>
      <td>0.00</td>
      <td>9.599391</td>
      <td>9.56</td>
    </tr>
    <tr>
      <th>12</th>
      <td>scipy_Newton-CG</td>
      <td>1863.146785</td>
      <td>0.00</td>
      <td>19.659599</td>
      <td>19.62</td>
    </tr>
    <tr>
      <th>4</th>
      <td>newton</td>
      <td>1863.146789</td>
      <td>0.00</td>
      <td>209.193209</td>
      <td>209.15</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Some relevant conclusions that we can extract from the above table and plots:</p>
<ul class="simple">
<li><p>The best methods in terms of optimality are: the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">scipy</span></code> implementations and our Gradient Descent, Newton and Quasi-Newton SR1 implementations.</p></li>
<li><p>The best methods in terms of time are: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> is th best, followed by our implementation of Stochastic Quasi-Newton SR1, Stochastic Gradient Descent with momentum and Stochastic Quasi-Newton BFGS.</p></li>
<li><p>The best method in terms of trade-off between optimality and time are: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> solver, <code class="docutils literal notranslate"><span class="pre">scipy</span></code> implementation of Quasi-Newton’s L-BFGS-B and SLSQP, followed by our implementations of Quasi-Newton’s BFGS and SR1, and later our Gradient Descent and Stochastic Newton.</p>
<ul>
<li><p>Among our implementations the best is Quasi-Newton’s BFGS and SR1</p></li>
<li><p>Among our stochastic implementations the best is Stochastic Quasi-Newton’s BFGS Method.</p></li>
</ul>
</li>
</ul>
</section>
<section id="analysis-of-the-sample-size-effect-in-some-of-the-stochastic-methods">
<h3><strong>Analysis of the sample size effect in some of the stochastic methods</strong><a class="headerlink" href="#analysis-of-the-sample-size-effect-in-some-of-the-stochastic-methods" title="Link to this heading">#</a></h3>
<p>In this section we are going to explore the effect of the sample size in the stochastic methods (or at least in some of them).</p>
<section id="stochastic-gradient-descent">
<h4><strong>Stochastic Gradient Descent</strong><a class="headerlink" href="#stochastic-gradient-descent" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_size_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;sample_size&#39;</span><span class="p">:</span> <span class="n">sample_size_grid</span><span class="p">}</span>

<span class="n">stochastic_gradient_descent</span> <span class="o">=</span> <span class="n">stochastic_gradient_descent_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> 
                                                                 <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">stochastic_gradient_descent</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_gradient_descent_best_params</span><span class="p">)</span>
<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_gradient_descent</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_size_grid</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
<span class="n">sgd_sample_size_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sgd_sample_size_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sample_size</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22</th>
      <td>9750</td>
      <td>2016.575250</td>
      <td>1.924896</td>
    </tr>
    <tr>
      <th>77</th>
      <td>9650</td>
      <td>2016.707930</td>
      <td>1.968579</td>
    </tr>
    <tr>
      <th>60</th>
      <td>9550</td>
      <td>2016.948602</td>
      <td>2.532017</td>
    </tr>
    <tr>
      <th>12</th>
      <td>9950</td>
      <td>2017.999126</td>
      <td>0.991965</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9850</td>
      <td>2018.105974</td>
      <td>0.876709</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sgd_sample_size_results</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sample_size</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>1150</td>
      <td>2071.985893</td>
      <td>0.330054</td>
    </tr>
    <tr>
      <th>72</th>
      <td>1550</td>
      <td>2074.434644</td>
      <td>0.638791</td>
    </tr>
    <tr>
      <th>33</th>
      <td>1650</td>
      <td>2079.663950</td>
      <td>1.353488</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1250</td>
      <td>2082.915842</td>
      <td>0.802284</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1350</td>
      <td>2085.161451</td>
      <td>0.539797</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;objective_value&#39;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;sample_size&#39;</span><span class="p">],</span> 
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;sample_size&#39;</span><span class="p">],</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size vs Objective Function&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size vs Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time (secs)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Sample Size&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1">#axes[0].set_xticks(np.round(np.linspace(np.min(sgd_sample_size_results[&#39;objective_value&#39;]), np.max(sgd_sample_size_results[&#39;objective_value&#39;]), 7)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;sample_size&#39;</span><span class="p">])</span><span class="o">+</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">800</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;objective_value&#39;</span><span class="p">]),</span> 
                             <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;objective_value&#39;</span><span class="p">])</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;sample_size&#39;</span><span class="p">])</span><span class="o">+</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">800</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sgd_sample_size_results</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent - Sample Size Comparison - Optimality and Time&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d5e31219f66fb1a8387aebc739246b717e3d889c5fb44b2236109dfe6708186a.png" src="_images/d5e31219f66fb1a8387aebc739246b717e3d889c5fb44b2236109dfe6708186a.png" />
</div>
</div>
<p>Some conclusion that can be obtained from the plot:</p>
<ul class="simple">
<li><p>It seems to be a negative relation between the sample size and the objective value. In general, if a greater sample sized is used to estimate the gradient, a better (lower in this case) objective value is found. But there are some small sample sizes (50-800) that work much better than other larger (1000-3000).</p></li>
<li><p>It seems to be a positive relation between the sample size and the computational time, but only for not large enough samples (50-5500). For large enough (5500-10000) the computational time is quite similar (has low variance) and is in average lower than the one of some smaller samples (3200-5000).</p></li>
</ul>
</section>
<section id="stochastic-newton-s-method">
<h4><strong>Stochastic Newton’s Method</strong><a class="headerlink" href="#stochastic-newton-s-method" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sn_sample_size_objective_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sn_sample_size_times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">grad_sample_size_grid</span> <span class="o">=</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">2500</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">6000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">6000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">]</span>
<span class="n">hess_sample_size_grid</span> <span class="o">=</span> <span class="p">[</span><span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span>  <span class="mi">700</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">]</span>

<span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grad_sample_size_grid</span><span class="p">,</span> <span class="n">hess_sample_size_grid</span><span class="p">):</span>

    <span class="n">stochastic_newton</span> <span class="o">=</span> <span class="n">Stochastic_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function</span><span class="p">,</span> 
                                             <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> 
                                             <span class="n">grad_sample_size</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_newton_best_params</span><span class="p">)</span>
    <span class="n">stochastic_newton</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
    <span class="n">sn_sample_size_objective_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">objective_optimal</span><span class="p">)</span>
    <span class="n">sn_sample_size_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stochastic_newton</span><span class="o">.</span><span class="n">run_time</span><span class="p">)</span>

<span class="n">sample_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grad_sample_size_grid</span><span class="p">,</span> <span class="n">hess_sample_size_grid</span><span class="p">))</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sn_sample_size_objective_values</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span> 
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sn_sample_size_times</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">sample_sizes</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size vs Objective Function&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size vs Time&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Objective value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time (secs)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Sample Size&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1">#axes[0].set_xticks(np.round(np.linspace(np.min(sgd_sample_size_results[&#39;objective_value&#39;]), np.max(sgd_sample_size_results[&#39;objective_value&#39;]), 7)))</span>
<span class="c1">#axes[0].set_yticks(np.arange(0, np.max(sgd_sample_size_results[&#39;sample_size&#39;])+1000, 1000))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">sn_sample_size_objective_values</span><span class="p">),</span> 
                             <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sn_sample_size_objective_values</span><span class="p">)</span><span class="o">+</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sn_sample_size_times</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Stochastic Gradient Descent - Sample Size Comparison - Optimality and Time&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/93e1b3372cc7126ed67ca3e7d5a06476f3872c854b7f29661e223cd2423ce178.png" src="_images/93e1b3372cc7126ed67ca3e7d5a06476f3872c854b7f29661e223cd2423ce178.png" />
</div>
</div>
<p>Some conclusions from the plot:</p>
<ul class="simple">
<li><p>When the combination of sample sizes used to estimated the gradient and the hessian, respectively, is large, the objective value is better (lower in this case).</p></li>
<li><p>There seem not to be a clear pattern that govern the relationship between this pairs of sample sizes and the computational time.</p></li>
</ul>
</section>
</section>
</section>
<section id="optimizing-penalized-logistic-regression">
<h2><strong>Optimizing Penalized Logistic Regression</strong><a class="headerlink" href="#optimizing-penalized-logistic-regression" title="Link to this heading">#</a></h2>
<p>In this section we are going to address penalized logistic regression, which is a variant of logistic regression that penalized the betas, enforcing them to fulfill some constraint.</p>
<p>The classic (Ridge) constraint is <span class="math notranslate nohighlight">\(|| \beta ||_2^2 = \sum_{j=0}^p \beta_j^2 = 0\)</span> and the idea is to bring some of the coefficient (betas) to zero, in order to add a sort of automatic feature selection to the model.</p>
<p>This is a popular idea in modern statistical learning. Ridge, Lasso and Elastic Net Regression are the greatest exponents of this technique, but it has been exported to other models, like Logistic Regression.</p>
<p>The problem to solve in this section is the following <strong>constrained</strong> problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{\beta} \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Max} \hspace{0.1cm} log(\mathcal{L}(\beta)) \\[0.3cm]
s.t.: \hspace{0.15cm}\small{\sum_{j=0}^p \beta_j^2 = 100}\end{split}\]</div>
<p>Which can be expressed as an <strong>unconstrained</strong> problem as follows:</p>
<div class="math notranslate nohighlight">
\[\widehat{\beta} \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Min} \hspace{0.1cm} - \Big[\hspace{0.08cm} log(\mathcal{L}(\beta)) - \lambda\cdot \small{\Big(\sum_{j=0}^p \beta_j^2 -100\Big)^2} \hspace{0.08cm}\Big]\]</div>
<p>Applying the conversion to Min problem and the matrix form, we have the following equivalent problem:</p>
<div class="math notranslate nohighlight">
\[\widehat{\beta} \hspace{0.1cm} = \hspace{0.1cm} arg \hspace{0.1cm} \underset{\beta}{Min} \hspace{0.1cm} -\Big[\hspace{0.05cm}   Y \cdot log(P_1) + (1-Y)\cdot log(1-P_1) \hspace{0.05cm} - \lambda\cdot (\beta^2 - 100)^2\Big]\]</div>
<p>The <strong>gradient</strong> of <span class="math notranslate nohighlight">\(f(\beta)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\nabla f(\beta) \hspace{0.06cm} = \hspace{0.06cm}   X^\prime \cdot (P_1-Y) + 2\lambda\cdot (\beta^2 - 100)\cdot 2\beta\]</div>
<p>The <strong>hessian</strong> of <span class="math notranslate nohighlight">\(f(\beta) = - log(\mathcal{L}(\beta))\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\nabla^2 f(\beta) \hspace{0.06cm} = \hspace{0.06cm}   X^\prime \cdot D \cdot X + 2\lambda \cdot 2 I_p\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[D= diag(P_1\cdot (1-P_1))\]</div>
<section id="id2">
<h3><strong>Initial elements</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>As we did before, we define the initial elements to address the problem computationally.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">minus_logL_penalized</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-15</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">sum1</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">sum2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>  <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">penalization</span> <span class="o">=</span> <span class="n">pen_lambda</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">betas</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">100</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">sum1</span> <span class="o">+</span> <span class="n">sum2</span> <span class="o">-</span> <span class="n">penalization</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">gradient_function_penalized</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">p1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">pen_lambda</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">betas</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">100</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">betas</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradient</span>

<span class="k">def</span> <span class="nf">hessian_function_penalized</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">betas</span><span class="p">)</span> 
    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">p1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">))</span>
    <span class="n">hessian</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">D</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pen_lambda</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hessian</span>

<span class="n">pen_lambda</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># in (0.7, 2) is enough.</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>As before, we will first solve the problem with <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> solvers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">benchmark_methods</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;scipy_BFGS&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_L-BFGS-B&#39;</span><span class="p">,</span> <span class="s1">&#39;scipy_SLSQP&#39;</span><span class="p">]</span>

<span class="c1"># BFGS := Broyden–Fletcher–Goldfarb–Shanno algorithm</span>
<span class="c1"># Newton-CG := Newton Conjugate Gradient algorithm</span>
<span class="c1"># L-BFGS-B := Limited memory BFGS</span>
<span class="c1"># SLSQP := Sequential Least Squares Programming </span>

<span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">benchmark_methods</span><span class="p">:</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">:</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">minus_logL_penalized</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> 
                                             <span class="n">jac</span><span class="o">=</span><span class="n">gradient_function_penalized</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="n">hessian_function_penalized</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">pen_lambda</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">minus_logL_penalized</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span><span class="p">))</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">benchmark_results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="c1"># Time: </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_BFGS&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Desired error not necessarily achieved due to precision loss.
  success: False
   status: 2
      fun: 2384.0578128540274
        x: [ 2.541e+00  2.550e+00 ...  6.324e-01  9.169e-01]
      nit: 56
      jac: [ 9.155e-05  9.155e-05 ... -3.052e-05  6.104e-05]
 hess_inv: [[ 7.753e-04  7.914e-05 ...  7.503e-05  1.077e-04]
            [ 7.914e-05  6.824e-04 ...  5.427e-05  1.858e-05]
            ...
            [ 7.503e-05  5.427e-05 ...  6.907e-04  3.014e-05]
            [ 1.077e-04  1.858e-05 ...  3.014e-05  7.279e-04]]
     nfev: 3493
     njev: 129
     time: 8.339475631713867
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> message: Optimization terminated successfully.
 success: True
  status: 0
     fun: 2384.0578133289528
       x: [ 2.541e+00  2.550e+00 ...  6.324e-01  9.168e-01]
     nit: 9
     jac: [ 4.150e-02 -3.019e-02 ... -2.178e-02 -3.583e-03]
    nfev: 16
    njev: 16
    nhev: 9
    time: 32.1147027015686
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_L-BFGS-B&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH
  success: True
   status: 0
      fun: 2384.0578128542115
        x: [ 2.541e+00  2.550e+00 ...  6.324e-01  9.169e-01]
      nit: 11
      jac: [ 1.364e-04  1.364e-04 ...  0.000e+00  3.183e-04]
     nfev: 351
     njev: 13
 hess_inv: &lt;26x26 LbfgsInvHessProduct with dtype=float64&gt;
     time: 1.153794765472412
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_SLSQP&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> message: Optimization terminated successfully
 success: True
  status: 0
     fun: 2384.057813545238
       x: [ 2.541e+00  2.550e+00 ...  6.324e-01  9.169e-01]
     nit: 25
     jac: [-1.343e-03 -5.524e-03 ...  6.500e-03  1.306e-02]
    nfev: 737
    njev: 25
    time: 1.7633354663848877
</pre></div>
</div>
</div>
</div>
<p>We can check easily if the constraint is fulfilled:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_BFGS&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>103.45900689903829
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_Newton-CG&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>103.45907721935968
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_L-BFGS-B&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>103.45900666588769
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">benchmark_results</span><span class="p">[</span><span class="s1">&#39;scipy_SLSQP&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>103.45905741002673
</pre></div>
</div>
</div>
</div>
<p>We can see that with <span class="math notranslate nohighlight">\(\lambda =1\)</span> constraint not totally satisfy, but is close to it.</p>
</section>
<section id="hpo">
<h3><strong>HPO</strong><a class="headerlink" href="#hpo" title="Link to this heading">#</a></h3>
<p>In this section we are going to solve the new problem only with one of our implemented methods, for the shake of brevity.</p>
<p>The chosen method is the Stochastic Quasi-Newton BFGS Method, which was the best among our Stochastic implementations in the previous section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pen_lambda</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">learning_rate_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>
<span class="n">phi_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="c1">#sigma_grid = np.logspace(-2, -1, num=50)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate_grid</span><span class="p">,</span> <span class="s1">&#39;phi&#39;</span><span class="p">:</span> <span class="n">phi_grid</span><span class="p">}</span>

<span class="n">stochastic_quasi_newton_BFGS</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL_penalized</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function_penalized</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function_penalized</span><span class="p">,</span> 
                                                              <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomSearch</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">stochastic_quasi_newton_BFGS</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span><span class="p">))</span>
<span class="n">stochastic_quasi_newton_BFGS_penalized_best_params</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span>
<span class="n">stochastic_quasi_newton_BFGS_penalized_results</span> <span class="o">=</span> <span class="n">random_search</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\fscielzo\AppData\Local\Temp\ipykernel_4396\3752159407.py:74: RuntimeWarning: invalid value encountered in scalar divide
  B[k+1] = B[k] - (M1 @ M1.T)/M2 + M3/M4
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized_results</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>learning_rate</th>
      <th>phi</th>
      <th>objective_value</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>0.000774</td>
      <td>0.82</td>
      <td>2414.395541</td>
      <td>3.294048</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.000742</td>
      <td>0.53</td>
      <td>2435.829487</td>
      <td>0.134308</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.000742</td>
      <td>0.97</td>
      <td>2437.416647</td>
      <td>0.796468</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.000626</td>
      <td>0.46</td>
      <td>2453.170528</td>
      <td>0.567720</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.000626</td>
      <td>0.96</td>
      <td>2453.426843</td>
      <td>0.834335</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.000599</td>
      <td>0.74</td>
      <td>2471.449251</td>
      <td>0.648206</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.000880</td>
      <td>0.46</td>
      <td>2474.771527</td>
      <td>6.856022</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.000880</td>
      <td>0.26</td>
      <td>2474.771527</td>
      <td>3.889722</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.000880</td>
      <td>0.15</td>
      <td>2474.771527</td>
      <td>4.042038</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.000918</td>
      <td>0.27</td>
      <td>2513.489051</td>
      <td>4.228621</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The penalty seems to affect the calculation time, increasing it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized_results</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.899277400970459
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_results</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.3946699798107147
</pre></div>
</div>
</div>
</div>
<p>We fit the method with the best parameters find during the HPO phase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized</span> <span class="o">=</span> <span class="n">Stochastic_Quasi_Newton_method</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">minus_logL_penalized</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="n">gradient_function_penalized</span><span class="p">,</span> <span class="n">hessian</span><span class="o">=</span><span class="n">hessian_function_penalized</span><span class="p">,</span> 
                                                   <span class="n">x_0</span><span class="o">=</span><span class="n">initial_betas</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">grad_sample_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">hess_sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">rule</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">stochastic_quasi_newton_BFGS_penalized_best_params</span><span class="p">)</span>
<span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">objective_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2414.3955411016636
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">x_optimal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 2.49175735,  2.48437229,  2.47435553, -0.02026137, -2.56829027,
       -1.23482926, -2.83378562,  1.23127567, -3.08038169,  2.84108034,
        2.18704128,  1.53785186,  0.32576063, -1.55905894,  0.5476085 ,
        0.57765483, -0.29964037, -3.0904155 ,  1.56870018, -2.21738445,
       -3.12750058,  2.17646045,  1.83412108, -0.33615271,  0.63636938,
        0.87384157])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">break_iter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>158
</pre></div>
</div>
</div>
</div>
<p>We can check if the constrain is fulfilled.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">x_optimal</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100.23194704760508
</pre></div>
</div>
</div>
</div>
</section>
<section id="analysis-of-lambda-influence-in-the-constraint-compliance">
<h3><strong>Analysis of lambda influence in the constraint compliance</strong><a class="headerlink" href="#analysis-of-lambda-influence-in-the-constraint-compliance" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reached_constraint_values</span><span class="p">,</span> <span class="n">reached_constraint_times</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">pen_lambda_grid</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>

<span class="k">for</span> <span class="n">pen_lambda</span> <span class="ow">in</span> <span class="n">pen_lambda_grid</span><span class="p">:</span>
    <span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">pen_lambda</span><span class="p">))</span>
    <span class="n">reached_constraint_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">x_optimal</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">reached_constraint_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stochastic_quasi_newton_BFGS_penalized</span><span class="o">.</span><span class="n">run_time</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">reached_constraint_values</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">pen_lambda_grid</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                  <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">reached_constraint_times</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">pen_lambda_grid</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Constraint value vs $</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Time vs $</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Constraint value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (secs)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Constraint&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pen_lambda_grid</span><span class="p">)</span><span class="o">+</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pen_lambda_grid</span><span class="p">)</span><span class="o">+</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Stochastic Newton</span><span class="se">\&#39;</span><span class="s1">s Method - Penalization Parameter - Reached Constraint and Time&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/855c3432d89f17783416e9628bffd438d9bc4c1bd0087086c49f8539a9c10107.png" src="_images/855c3432d89f17783416e9628bffd438d9bc4c1bd0087086c49f8539a9c10107.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zoom_limit</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">reached_constraint_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">pen_lambda_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                  <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">reached_constraint_times</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">pen_lambda_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Constraint value vs $</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Time vs $</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">lambda$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Constraint value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Time (secs)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Constraint&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">reached_constraint_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">]),</span> 
                                      <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">reached_constraint_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">])</span><span class="o">+</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pen_lambda_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">])</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pen_lambda_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">zoom_limit</span><span class="p">])</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Zoom - Stochastic Newton</span><span class="se">\&#39;</span><span class="s1">s Method - Penalization Parameter - Reached Constraint and Time&#39;</span><span class="p">,</span> 
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7d55b44adc74928011173a1c71f323c11c108da29a99c64c2332836f025aef03.png" src="_images/7d55b44adc74928011173a1c71f323c11c108da29a99c64c2332836f025aef03.png" />
</div>
</div>
<p>Conclusions:</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(\lambda\)</span> is in <span class="math notranslate nohighlight">\((0,4]\)</span> the convergence is slower than when is <span class="math notranslate nohighlight">\(&gt; 4\)</span>, even for large values of lambda, such as <span class="math notranslate nohighlight">\(1000\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(\lambda &gt;= 3\)</span>  the difference between the reached constraint and the tue is less than <span class="math notranslate nohighlight">\(0.1\)</span>. And for <span class="math notranslate nohighlight">\(\lambda &gt;7\)</span> there are non significative differences, in other words, the constraint is satisfy.</p></li>
<li><p>Even for small <span class="math notranslate nohighlight">\(\lambda\)</span> like <span class="math notranslate nohighlight">\(0.5\)</span>, <span class="math notranslate nohighlight">\(0.7\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, the constraint is almost fulfilled (the difference is less than <span class="math notranslate nohighlight">\(0.5\)</span>).</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Numerical Optimization</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements"><strong>Requirements</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-logistic-regression"><strong>Binary Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#framework"><strong>Framework</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-define-p-r-x-i"><strong>How to define <span class="math notranslate nohighlight">\(P_r(x_i)\)</span> ?</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-approach-linear-regression"><strong>Naive approach: Linear Regression</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-approach"><strong>Logistic Regression approach</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-estimation"><strong>Model estimation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-predictions"><strong>Model predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-p-r-x-i">Predicting <span class="math notranslate nohighlight">\(P_r(x_i)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-mathcal-y">Predicting <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimization-problem-to-solve">The optimization problem to solve</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization-algorithms"><strong>Numerical optimization algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Framework</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-conditions"><strong>Optimality conditions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-and-global-solutions"><strong>Local and Global solutions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-important-numerical-optimization-algorithms"><strong>Most important Numerical Optimization algorithms</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-logistic-regression"><strong>Optimizing Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initial-elements"><strong>Initial elements</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-and-scipy-optimization"><strong><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-implementation-of-numerical-optimization-algorithms"><strong>Self-implementation of numerical optimization algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-method"><strong>Gradient Descent Method</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-descent-method">Coordinate Descent Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-mini-batch">Stochastic Gradient Descent (Mini-Batch)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-method">Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-newton-method">Stochastic Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-newton-method">Quasi-Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-quasi-newton-method">Stochastic Quasi-Newton Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyper-parameters-optimization-hpo"><strong>Hyper-parameters Optimization (HPO)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-gradient-descent"><strong>HPO for Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-gradient-descent-mini-batch"><strong>HPO for Stochastic Gradient Descent (Mini-Batch)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-coordinate-descent"><strong>HPO for Coordinate Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-newton"><strong>HPO for Stochastic Newton</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-quasi-newton"><strong>HPO for Quasi Newton</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo-for-stochastic-quasi-newton"><strong>HPO for Stochastic Quasi Newton</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-convergence"><strong>Analysis of convergence</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-gradient-descent"><strong>Convergence of Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-coordinate-descent"><strong>Convergence of Coordinate Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-gradient-descent"><strong>Convergence of Stochastic Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-gradient-descent-with-momentum"><strong>Convergence of Stochastic Gradient Descent with Momentum</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-newton-s-method"><strong>Convergence of Newton’s Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-stochastic-newton-s-method"><strong>Convergence of Stochastic Newton’s Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-quasi-newton-s-bfgs-method"><strong>Convergence of Quasi-Newton’s BFGS Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-of-quasi-newton-s-sr1-method"><strong>Convergence of Quasi-Newton’s SR1 Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-convergence-of-quasi-newton-s-bfgs-method"><strong>Stochastic Convergence of Quasi-Newton’s BFGS Method</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-convergence-of-quasi-newton-s-sr1-method"><strong>Stochastic Convergence of Quasi-Newton’s SR1 Method</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-comparison-optimal-values-vs-times"><strong>Methods Comparison: Optimal values vs Times</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-the-sample-size-effect-in-some-of-the-stochastic-methods"><strong>Analysis of the sample size effect in some of the stochastic methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent"><strong>Stochastic Gradient Descent</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-newton-s-method"><strong>Stochastic Newton’s Method</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-penalized-logistic-regression"><strong>Optimizing Penalized Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Initial elements</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><code class="docutils literal notranslate"><span class="pre">Sklearn</span></code> and <code class="docutils literal notranslate"><span class="pre">Scipy</span></code> optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hpo"><strong>HPO</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-lambda-influence-in-the-constraint-compliance"><strong>Analysis of lambda influence in the constraint compliance</strong></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Fabio Scielzo Ortiz
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>